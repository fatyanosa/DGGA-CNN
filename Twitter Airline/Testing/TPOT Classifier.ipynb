{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIWlEhkb6RVQ"
   },
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2996,
     "status": "ok",
     "timestamp": 1602194424745,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "wtGI1P4u6I-J"
   },
   "outputs": [],
   "source": [
    "class utility:\n",
    "\n",
    "    def append_df_to_excel(self, df, excel_path):\n",
    "        if path.isfile(excel_path):\n",
    "            df_excel = pd.read_excel(excel_path)\n",
    "            result = pd.concat([df_excel, df], ignore_index=True)\n",
    "            result.to_excel(excel_path, index=False)\n",
    "        else:\n",
    "            df.to_excel(excel_path, index=False)\n",
    "\n",
    "    def read_CSV(self, filename):\n",
    "        df = pd.read_csv(filename, encoding= 'unicode_escape')\n",
    "        return df\n",
    "\n",
    "    def get_text_label(self, df):\n",
    "        texts = []  # list of text samples\n",
    "        labels = []  # list of label ids\n",
    "        for index, row in df.iterrows():\n",
    "            if isinstance(row['text'], float):\n",
    "                texts.append(str(row['text']))\n",
    "            else:\n",
    "                texts.append(row['text'])\n",
    "\n",
    "            labels.append(row['sentiment'])\n",
    "\n",
    "        return texts, labels\n",
    "\n",
    "    def tokenize_texts(self, texts):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def padding_texts(self, texts, maxlen):\n",
    "\n",
    "        texts = tf.keras.preprocessing.sequence.pad_sequences(texts, padding='post', maxlen=maxlen)\n",
    "\n",
    "        return texts\n",
    "\n",
    "    def get_testing_metric(self, y_test, y_pred):\n",
    "        accuracyScore = accuracy_score(y_test, y_pred)\n",
    "        precisionScore= precision_score(y_test, y_pred)\n",
    "        recallScore = recall_score(y_test, y_pred)\n",
    "        f1Score = f1_score(y_test, y_pred)\n",
    "\n",
    "        return accuracyScore, precisionScore, recallScore, f1Score\n",
    "\n",
    "    def write_df_csv(self, df, out_path):\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "    def create_embedding_matrix(self, filepath, word_index, embedding_dim):\n",
    "        vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "        with open(filepath, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                word, *vector = line.split()\n",
    "                if word in word_index:\n",
    "                    idx = word_index[word]\n",
    "                    embedding_matrix[idx] = np.array(\n",
    "                        vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def get_max_length_of_sentences(self, texts):\n",
    "        maxlength = 0\n",
    "        for text in texts:\n",
    "            if (len(text.split()) > maxlength):\n",
    "                maxlength = len(text.split())\n",
    "\n",
    "        return maxlength\n",
    "\n",
    "    def get_training_trial_data(self, textsTraining, textsTrial, labelsTraining, labelsTrial):\n",
    "        textsTraining, textsTesting = np.asarray(textsTraining), np.asarray(textsTrial)\n",
    "        y_train, y_val = np.asarray(labelsTraining), np.asarray(labelsTrial)\n",
    "\n",
    "        # Tokenize words\n",
    "        tokenizer = self.tokenize_texts(textsTraining)\n",
    "        X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "        X_val = tokenizer.texts_to_sequences(textsTesting)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # get maxlen\n",
    "        maxlen = self.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "        X_train = self.padding_texts(X_train, maxlen)\n",
    "        X_val = self.padding_texts(X_val, maxlen)\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "    def get_X_Y_data(self, textsTraining, labelsTraining):\n",
    "        textsTraining = np.asarray(textsTraining)\n",
    "        y_train = np.asarray(labelsTraining)\n",
    "\n",
    "        # Tokenize words\n",
    "        tokenizer = self.tokenize_texts(textsTraining)\n",
    "        X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # get maxlen\n",
    "        maxlen = self.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "        X_train = self.padding_texts(X_train, maxlen)\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    def Average(self, list):\n",
    "        return sum(list) / len(list)\n",
    "    \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2989,
     "status": "ok",
     "timestamp": 1602194424746,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "nmkcOBiS62Hx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def cnn_model(self, vocab_size, maxlen, embedding_matrix, indiv, path):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        conv_idx = dense_idx = dropout_idx = maxpooling_idx = 0\n",
    "        for layer in path:\n",
    "            if layer == 'embedding_layer':\n",
    "                model.add(\n",
    "                    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=indiv['output_dim'],\n",
    "                                     weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "            elif layer == 'convolutional_layer':\n",
    "                conv_idx += 1\n",
    "                model.add(tf.keras.layers.Conv1D(indiv['num_filters' + str(conv_idx)], indiv['kernel_size' + str(conv_idx)],\n",
    "                                        kernel_initializer=indiv['conv_init_mode' + str(conv_idx)],\n",
    "                                        activation=indiv['conv_activation_func' + str(conv_idx)],\n",
    "                                        kernel_constraint=tf.keras.constraints.max_norm(indiv['conv_weight_constraint' + str(conv_idx)]),\n",
    "                                        data_format='channels_first'))\n",
    "            elif layer == 'dense_layer':\n",
    "                dense_idx += 1\n",
    "                model.add(tf.keras.layers.Dense(indiv['neurons' + str(dense_idx)],\n",
    "                                       kernel_initializer=indiv['dense_init_mode' + str(dense_idx)],\n",
    "                                       activation=indiv['dense_activation_func' + str(dense_idx)],\n",
    "                                       kernel_constraint=tf.keras.constraints.max_norm(indiv['dense_weight_constraint' + str(dense_idx)])))\n",
    "            elif layer == 'dropout_layer':\n",
    "                dropout_idx += 1\n",
    "                model.add(tf.keras.layers.Dropout(indiv['dropout_rate' + str(dropout_idx)]))\n",
    "            elif layer == 'maxpooling_layer':\n",
    "                maxpooling_idx += 1\n",
    "                model.add(tf.keras.layers.MaxPooling1D(indiv['pool_size' + str(maxpooling_idx)]))\n",
    "            elif layer == 'global_maxpooling_layer':\n",
    "                model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "            elif layer == 'output_layer':\n",
    "                model.add(tf.keras.layers.Dense(1, kernel_initializer=indiv['output_init_mode'], activation='sigmoid'))\n",
    "\n",
    "        if indiv['optimizer'] == 'sgd':\n",
    "            opt = tf.keras.optimizers.SGD(lr=indiv['learning_rate'], momentum=indiv['momentum'], decay=0.0,\n",
    "                                 nesterov=False)\n",
    "        elif indiv['optimizer'] == 'rmsprop':\n",
    "            opt = tf.keras.optimizers.RMSprop(lr=indiv['learning_rate'], rho=0.9, epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adagrad':\n",
    "            opt = tf.keras.optimizers.Adagrad(lr=indiv['learning_rate'], epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adadelta':\n",
    "            opt = tf.keras.optimizers.Adadelta(lr=indiv['learning_rate'], rho=0.95, epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adam':\n",
    "            opt = tf.keras.optimizers.Adam(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                  decay=0.0, amsgrad=False)\n",
    "        elif indiv['optimizer'] == 'adamax':\n",
    "            opt = tf.keras.optimizers.Adamax(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                    decay=0.0)\n",
    "        elif indiv['optimizer'] == 'nadam':\n",
    "            opt = tf.keras.optimizers.Nadam(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                   schedule_decay=0.004)\n",
    "\n",
    "        util = utility()\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[util.f1_m])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 2981,
     "status": "ok",
     "timestamp": 1602194424747,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "H2LjAGNP6s-R",
    "outputId": "c1971e28-ae59-459b-9b8f-7402002db19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "training_path = 'trainval.csv'\n",
    "testing_path = 'test.csv'\n",
    "root_path = '/lab/dbms/fatyanosa'\n",
    "datasetPath = '{}/Dataset/Twitter US Airline Sentiment/'.format(root_path)\n",
    "resultsPath = '{}/Server1/Twitter US Airline Sentiment/Paper DGGA-CNN/Results/'.format(root_path)\n",
    "testing_name = \"TPOT_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2972,
     "status": "ok",
     "timestamp": 1602194424747,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "zM0h-Ioi6rFz"
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from deap import base\n",
    "# import warnings; warnings.simplefilter('ignore')\n",
    "# import timeit\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     util = utility()\n",
    "#     # Read data\n",
    "#     dfTraining = util.read_CSV(datasetPath + training_path)\n",
    "    \n",
    "#     # Read trial data\n",
    "#     dfTrial = util.read_CSV(datasetPath + trial_path)\n",
    "\n",
    "#     textsTraining, labelsTraining = util.get_text_label(dfTraining)\n",
    "#     textsTrial, labelsTrial = util.get_text_label(dfTrial)\n",
    "#     cfold = {}\n",
    "\n",
    "#     X_train, X_val, y_train, y_val, vocab_size, maxlen, embedding_matrix = util.get_training_trial_data(\n",
    "#         textsTraining, labelsTraining, textsTrial, labelsTrial, glovePath)\n",
    "                  \n",
    "#     from tpot import TPOTClassifier\n",
    "#     tpot = TPOTClassifier(generations=100, population_size=30, mutation_rate=0.2, crossover_rate=0.8, early_stop=10, scoring='f1')\n",
    "#     start_time = timeit.default_timer()\n",
    "#     tpot.fit(X_train, y_train)\n",
    "#     elapsed = timeit.default_timer() - start_time\n",
    "#     winning_pipe=tpot.fitted_pipeline_\n",
    "#     score=tpot.score(X_val, y_val) \n",
    "#     tpot.export(resultsPath+'tpot_mnist_pipeline.py')\n",
    "#     elapsed = elapsed/60\n",
    "#     print('Time:', elapsed)\n",
    "#     print('Score:', score)   \n",
    "#     print('Winning pipeline:', winning_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 254219,
     "status": "ok",
     "timestamp": 1602194676003,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "gE6pvb2xNWZk",
    "outputId": "18c73f0f-118c-4690-90c1-6dfa4269bac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.29472041130066\n",
      "0.6351842241826673\n",
      "8.304448127746582\n",
      "0.6335078534031414\n",
      "8.307457208633423\n",
      "0.6257098606091895\n",
      "8.287773370742798\n",
      "0.6452282157676348\n",
      "8.285102605819702\n",
      "0.6378772112382933\n",
      "8.30013656616211\n",
      "0.6352819451629592\n",
      "8.31404185295105\n",
      "0.6315245478036176\n",
      "8.350783109664917\n",
      "0.6315240083507307\n",
      "8.330127954483032\n",
      "0.6303219106957424\n",
      "8.293934345245361\n",
      "0.6276762402088774\n",
      "8.28831171989441\n",
      "0.6285119667013528\n",
      "8.251240491867065\n",
      "0.6298802706923478\n",
      "8.300720930099487\n",
      "0.6246056782334386\n",
      "8.347569465637207\n",
      "0.6238821672803787\n",
      "8.335811138153076\n",
      "0.6340956340956341\n",
      "8.32011365890503\n",
      "0.6342472840144853\n",
      "8.307449102401733\n",
      "0.6378772112382933\n",
      "8.32282018661499\n",
      "0.6292834890965732\n",
      "8.296455144882202\n",
      "0.6416666666666666\n",
      "8.334139823913574\n",
      "0.6244106862231535\n",
      "8.335028171539307\n",
      "0.6327800829875518\n",
      "8.345573663711548\n",
      "0.6267423851316468\n",
      "8.364209651947021\n",
      "0.6384734399174833\n",
      "8.313401460647583\n",
      "0.631083202511774\n",
      "8.327104091644287\n",
      "0.6350858927641854\n",
      "8.321603536605835\n",
      "0.6436420722135009\n",
      "8.299376010894775\n",
      "0.6350515463917527\n",
      "8.306139945983887\n",
      "0.6193682030036252\n",
      "8.289093971252441\n",
      "0.6327272727272727\n",
      "8.306442022323608\n",
      "0.6233766233766234\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from copy import copy\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# features = tpot_data.drop('target', axis=1)\n",
    "# training_features, testing_features, training_target, testing_target = \\\n",
    "#             train_test_split(features, tpot_data['target'], random_state=None)\n",
    "util = utility()\n",
    "n_run = 30\n",
    "\n",
    "# Read data\n",
    "dfTraining = util.read_CSV(datasetPath + training_path)\n",
    "\n",
    "# Read trial data\n",
    "dfTest = util.read_CSV(datasetPath + testing_path)\n",
    "\n",
    "textsTraining, labelsTraining = util.get_text_label(dfTraining)\n",
    "textsTest, labelsTest = util.get_text_label(dfTest)\n",
    "\n",
    "X_train, X_test, y_train, y_test = util.get_training_trial_data(\n",
    "    textsTraining, textsTest, labelsTraining, labelsTest)\n",
    "\n",
    "# Create Testing Results\n",
    "f = open(resultsPath + testing_name + \".csv\", \"w+\")\n",
    "f.write(\"i,accuracy,precision,recall,f1Score,time\\n\")\n",
    "f.close()\n",
    "for i in range(0, n_run):\n",
    "    then = time.time()\n",
    "    exported_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    GradientBoostingClassifier(learning_rate=0.1, max_depth=8, max_features=0.6000000000000001, min_samples_leaf=2, min_samples_split=6, n_estimators=100, subsample=1.0))\n",
    "    \n",
    "    exported_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = exported_pipeline.predict(X_test)\n",
    "\n",
    "    # CNN metrics\n",
    "    accuracyScore, precisionScore, recallScore, f1Score = util.get_testing_metric(y_test, y_pred)\n",
    "\n",
    "    now = time.time()\n",
    "    diff = now - then\n",
    "    print(diff)\n",
    "    print(accuracyScore)\n",
    "\n",
    "    # save testing data\n",
    "    f = open(resultsPath + testing_name + \".csv\", 'a')\n",
    "    f.write(str(i + 1)\n",
    "            + ',' + str(accuracyScore)\n",
    "            + ',' + str(precisionScore)\n",
    "            + ',' + str(recallScore)\n",
    "            + ',' + str(f1Score)\n",
    "            + ',' + str(diff) + '\\n')\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPlLb8LcaCOWEEtz1b3R/7",
   "collapsed_sections": [],
   "name": "TPOT Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
