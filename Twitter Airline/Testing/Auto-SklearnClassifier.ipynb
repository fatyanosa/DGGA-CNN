{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "53XcMzZR8xsb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from os import path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QCSYQmxC8zCx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from os import path\n",
    "import csv\n",
    "\n",
    "class utility:\n",
    "\n",
    "    def append_df_to_excel(self, df, excel_path):\n",
    "        if path.isfile(excel_path):\n",
    "            df_excel = pd.read_excel(excel_path)\n",
    "            result = pd.concat([df_excel, df], ignore_index=True)\n",
    "            result.to_excel(excel_path, index=False)\n",
    "        else:\n",
    "            df.to_excel(excel_path, index=False)\n",
    "\n",
    "    def read_CSV(self, filename):\n",
    "        df = pd.read_csv(filename, encoding= 'unicode_escape')\n",
    "        return df\n",
    "\n",
    "    def get_text_label(self, df):\n",
    "        texts = []  # list of text samples\n",
    "        labels = []  # list of label ids\n",
    "        for index, row in df.iterrows():\n",
    "            if isinstance(row['text'], float):\n",
    "                texts.append(str(row['text']))\n",
    "            else:\n",
    "                texts.append(row['text'])\n",
    "\n",
    "            labels.append(row['sentiment'])\n",
    "\n",
    "        return texts, labels\n",
    "\n",
    "    def tokenize_texts(self, texts):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def padding_texts(self, texts, maxlen):\n",
    "\n",
    "        texts = tf.keras.preprocessing.sequence.pad_sequences(texts, padding='post', maxlen=maxlen)\n",
    "\n",
    "        return texts\n",
    "\n",
    "    def get_testing_metric(self, y_test, y_pred):\n",
    "        accuracyScore = accuracy_score(y_test, y_pred)\n",
    "        precisionScore= precision_score(y_test, y_pred)\n",
    "        recallScore = recall_score(y_test, y_pred)\n",
    "        f1Score = f1_score(y_test, y_pred)\n",
    "\n",
    "        return accuracyScore, precisionScore, recallScore, f1Score\n",
    "\n",
    "    def write_df_csv(self, df, out_path):\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "    def create_embedding_matrix(self, filepath, word_index, embedding_dim):\n",
    "        vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "        with open(filepath, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                word, *vector = line.split()\n",
    "                if word in word_index:\n",
    "                    idx = word_index[word]\n",
    "                    embedding_matrix[idx] = np.array(\n",
    "                        vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def get_max_length_of_sentences(self, texts):\n",
    "        maxlength = 0\n",
    "        for text in texts:\n",
    "            if (len(text.split()) > maxlength):\n",
    "                maxlength = len(text.split())\n",
    "\n",
    "        return maxlength\n",
    "\n",
    "    def get_training_trial_data(self, textsTraining, textsTrial, labelsTraining, labelsTrial):\n",
    "        textsTraining, textsTesting = np.asarray(textsTraining), np.asarray(textsTrial)\n",
    "        y_train, y_val = np.asarray(labelsTraining), np.asarray(labelsTrial)\n",
    "\n",
    "        # Tokenize words\n",
    "        tokenizer = self.tokenize_texts(textsTraining)\n",
    "        X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "        X_val = tokenizer.texts_to_sequences(textsTesting)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # get maxlen\n",
    "        maxlen = self.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "        X_train = self.padding_texts(X_train, maxlen)\n",
    "        X_val = self.padding_texts(X_val, maxlen)\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "    def get_X_Y_data(self, textsTraining, labelsTraining):\n",
    "        textsTraining = np.asarray(textsTraining)\n",
    "        y_train = np.asarray(labelsTraining)\n",
    "\n",
    "        # Tokenize words\n",
    "        tokenizer = self.tokenize_texts(textsTraining)\n",
    "        X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # get maxlen\n",
    "        maxlen = self.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "        X_train = self.padding_texts(X_train, maxlen)\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    def Average(self, list):\n",
    "        return sum(list) / len(list)\n",
    "    \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 12408,
     "status": "ok",
     "timestamp": 1592426064759,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "qiuH4jil823h",
    "outputId": "e193ff31-e2d4-4ac5-a265-2fdb79dbcfc9"
   },
   "outputs": [],
   "source": [
    "training_path = 'trainval.csv'\n",
    "testing_path = 'test.csv'\n",
    "root_path = '/lab/dbms/fatyanosa'\n",
    "datasetPath = '{}/Dataset/Twitter US Airline Sentiment/'.format(root_path)\n",
    "resultsPath = '{}/Server1/Twitter US Airline Sentiment/Paper DGGA-CNN/Results/'.format(root_path)\n",
    "testing_name = \"Auto-Sklearn_3hr1_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "f8svPD6t85UZ"
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import warnings; warnings.simplefilter('ignore')\n",
    "# import timeit\n",
    "# import autosklearn.classification\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     util = utility()\n",
    "#     n_run = 1\n",
    "\n",
    "#     # Read data\n",
    "#     dfTraining = util.read_CSV(datasetPath + training_path)\n",
    "    \n",
    "#     # Read trial data\n",
    "#     dfTrial = util.read_CSV(datasetPath + testing_path)\n",
    "\n",
    "#     textsTraining, labelsTraining = util.get_text_label(dfTraining)\n",
    "#     textsTrial, labelsTrial = util.get_text_label(dfTrial)\n",
    "\n",
    "#     X_train, X_val, y_train, y_val = util.get_training_trial_data(\n",
    "#     textsTraining, textsTrial, labelsTraining, labelsTrial)\n",
    "    \n",
    "#     # Create Testing Results\n",
    "#     f = open(resultsPath + testing_name + \".csv\", \"a+\")\n",
    "#     f.write(\"i,score,time\\n\")\n",
    "#     f.close()\n",
    "\n",
    "#     scorer = autosklearn.metrics.make_scorer(\n",
    "#           'f1_score',\n",
    "#           f1_score\n",
    "#     )\n",
    "\n",
    "#     for i in range(0, n_run):\n",
    "#       start_time = timeit.default_timer()      \n",
    "\n",
    "#       cls = autosklearn.classification.AutoSklearnClassifier()\n",
    "      \n",
    "#       score = cls.fit(X_train, y_train, metric=scorer).score(X_val, y_val)\n",
    "#       pickle.dump(cls, open(resultsPath + testing_name + str(i + 1) + '.pickle', 'wb'))\n",
    "#       elapsed = timeit.default_timer() - start_time    \n",
    "\n",
    "#       elapsed = elapsed\n",
    "#       # print('Time:', elapsed)\n",
    "#       # print('Score:', score)\n",
    "\n",
    "#       # save testing data\n",
    "#       f = open(resultsPath + testing_name + \".csv\", 'a')\n",
    "#       f.write(str(i + 1)\n",
    "#             + ',' + str(score)\n",
    "#             + ',' + str(elapsed)\n",
    "#             + '\\n')\n",
    "#       f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 142787,
     "status": "ok",
     "timestamp": 1592426195160,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "Z2UtNalBF13Y",
    "outputId": "dda5a32d-1902-4a39-8a82-0824b1cb2169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autosklearn.util.backend:Could not delete output dir: /tmp/autosklearn_output_62725_9122\n",
      "WARNING:autosklearn.util.backend:Could not delete tmp dir: /tmp/autosklearn_tmp_62725_9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568306010928961\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import time\n",
    "import autosklearn.classification\n",
    "\n",
    "util = utility()\n",
    "n_run = 30\n",
    "\n",
    "# Read data\n",
    "dfTraining = util.read_CSV(datasetPath + training_path)\n",
    "\n",
    "# Read trial data\n",
    "dfTest = util.read_CSV(datasetPath + testing_path)\n",
    "\n",
    "textsTraining, labelsTraining = util.get_text_label(dfTraining)\n",
    "textsTest, labelsTest = util.get_text_label(dfTest)\n",
    "\n",
    "X_train, X_test, y_train, y_test = util.get_training_trial_data(\n",
    "    textsTraining, textsTest, labelsTraining, labelsTest)\n",
    "\n",
    "# Create Testing Results\n",
    "f = open(resultsPath + testing_name + \".csv\", \"w+\")\n",
    "f.write(\"i,accuracy,precision,recall,f1Score,time\\n\")\n",
    "f.close()\n",
    "for i in range(0, n_run):\n",
    "    then = time.time()\n",
    "    \n",
    "    with open(resultsPath + 'Auto-Sklearn_3hr1' + '.pickle','rb') as f:\n",
    "        loaded_model = pickle.load(f) \n",
    "        y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "        # CNN metrics\n",
    "        accuracyScore, precisionScore, recallScore, f1Score = util.get_testing_metric(y_test, y_pred)\n",
    "\n",
    "        now = time.time()\n",
    "        diff = now - then\n",
    "        print(accuracyScore)\n",
    "\n",
    "        # save testing data\n",
    "        f = open(resultsPath + testing_name + \".csv\", 'a')\n",
    "        f.write(str(i + 1)\n",
    "              + ',' + str(accuracyScore)\n",
    "              + ',' + str(precisionScore)\n",
    "              + ',' + str(recallScore)\n",
    "              + ',' + str(f1Score)\n",
    "              + ',' + str(diff) + '\\n')\n",
    "        f.close()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+azmkRiwYZiI/LSl+Bg5H",
   "collapsed_sections": [],
   "name": "Auto-SklearnClassifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
