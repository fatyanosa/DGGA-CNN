{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZg_QFpe7W-B"
   },
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "colab_type": "code",
    "id": "_OHVp0iz7a9O",
    "outputId": "fbe1d9ae-3b55-4f5f-c4de-46db62ddcbb2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class utility:\n",
    "\n",
    "    def read_CSV(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "\n",
    "    def get_text_label(self, df):\n",
    "        texts = []  # list of text samples\n",
    "        labels = []  # list of label ids\n",
    "        for index, row in df.iterrows():\n",
    "            if isinstance(row['sentence'], float):\n",
    "                texts.append(str(row['sentence']))\n",
    "            else:\n",
    "                texts.append(row['sentence'])\n",
    "\n",
    "            labels.append(row['label'])\n",
    "\n",
    "        return texts, labels\n",
    "\n",
    "    def tokenize_texts(self, texts):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def padding_texts(self, texts, maxlen):\n",
    "\n",
    "        texts = tf.keras.preprocessing.sequence.pad_sequences(texts, padding='post', maxlen=maxlen)\n",
    "\n",
    "        return texts\n",
    "\n",
    "    def get_metric(self, y_true, y_pred):\n",
    "        accuracyScore = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        # binary: Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "        precisionScoreBinary = precision_score(y_true, y_pred, average='binary')\n",
    "        recallScoreBinary = recall_score(y_true, y_pred, average='binary')\n",
    "        f1ScoreBinary = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "        # Macro: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "        f1ScoreMacro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        # Micro: Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "        f1ScoreMicro = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "        # Weighted: Calculate metrics for each label, and find their average weighted by support\n",
    "        # (the number of true instances for each label).\n",
    "        # This alters ‘macro’ to account for label imbalance;\n",
    "        # it can result in an F-score that is not between precision and recall.\n",
    "        f1ScoreWeighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "        # None: the scores for each class are returned\n",
    "        f1Score = f1_score(y_true, y_pred, average=None)\n",
    "        # , f1ScoreMacro, f1ScoreMicro, f1ScoreWeighted, f1Score\n",
    "\n",
    "        # self.print_metric(accuracyScore, precisionScoreBinary, recallScoreBinary, f1ScoreBinary)\n",
    "        return accuracyScore, precisionScoreBinary, recallScoreBinary, f1ScoreBinary\n",
    "\n",
    "    def print_metric(self, accuracyScore, precisionScoreBinary, recallScoreBinary, f1ScoreBinary):\n",
    "        print(\"Accuracy: \" + str(accuracyScore))\n",
    "        print(\"Precision: \" + str(precisionScoreBinary))\n",
    "        print(\"Recall: \" + str(recallScoreBinary))\n",
    "        print(\"F1-Score: \" + str(f1ScoreBinary))\n",
    "        print(str(accuracyScore) + \",\" + str(precisionScoreBinary) + \",\" + str(recallScoreBinary) + \",\" + str(\n",
    "            f1ScoreBinary))\n",
    "\n",
    "    def get_testing_metric(self, y_test, y_pred):\n",
    "        # metric for Testing Data\n",
    "        # print(\"Testing Data\")\n",
    "        accuracyScore, precisionScoreBinary, recallScoreBinary, f1ScoreBinary = self.get_metric(y_test, y_pred)\n",
    "        # print()\n",
    "\n",
    "        return accuracyScore, precisionScoreBinary, recallScoreBinary, f1ScoreBinary\n",
    "\n",
    "    def write_df_csv(self, df, out_path):\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "    def create_embedding_matrix(self, filepath, word_index, embedding_dim):\n",
    "        vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "        with open(filepath, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                word, *vector = line.split()\n",
    "                if word in word_index:\n",
    "                    idx = word_index[word]\n",
    "                    embedding_matrix[idx] = np.array(\n",
    "                        vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def get_max_length_of_sentences(self, texts):\n",
    "        maxlength = 0\n",
    "        for text in texts:\n",
    "            if (len(text.split()) > maxlength):\n",
    "                maxlength = len(text.split())\n",
    "\n",
    "        return maxlength\n",
    "\n",
    "    def get_training_trial_data(self, textsTraining, labelsTraining, textsTrial, labelsTrial, glovePath):\n",
    "        textsTraining, textsTesting = np.asarray(textsTraining), np.asarray(textsTrial)\n",
    "        y_train, y_val = np.asarray(labelsTraining), np.asarray(labelsTrial)\n",
    "\n",
    "        # Tokenize words\n",
    "        tokenizer = self.tokenize_texts(textsTraining)\n",
    "        X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "        X_val = tokenizer.texts_to_sequences(textsTesting)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # get maxlen\n",
    "        maxlen = self.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "        X_train = self.padding_texts(X_train, maxlen)\n",
    "        X_val = self.padding_texts(X_val, maxlen)\n",
    "\n",
    "        embedding_matrix = []\n",
    "        embedding_matrix.append(self.create_embedding_matrix(glovePath[0], tokenizer.word_index, 50))\n",
    "        embedding_matrix.append(self.create_embedding_matrix(glovePath[1], tokenizer.word_index, 100))\n",
    "        embedding_matrix.append(self.create_embedding_matrix(glovePath[2], tokenizer.word_index, 200))\n",
    "        embedding_matrix.append(self.create_embedding_matrix(glovePath[3], tokenizer.word_index, 300))\n",
    "\n",
    "        return X_train, X_val, y_train, y_val, vocab_size, maxlen, embedding_matrix\n",
    "\n",
    "    def Average(self, list):\n",
    "        return sum(list) / len(list)\n",
    "\n",
    "    def recall_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Asv1GED6mQqR"
   },
   "source": [
    "# Finite State Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUYcMCfemUm_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def FSM():\n",
    "    fsm = {}\n",
    "    fsm[0] = {'src': 0, 'dst': 1, 'layer': 'embedding_layer', 'next_path': [1]}\n",
    "    fsm[1] = {'src': 1, 'dst': 2, 'layer': 'convolutional_layer', 'next_path': [2, 3, 5]}\n",
    "    fsm[2] = {'src': 2, 'dst': 2, 'layer': 'convolutional_layer', 'next_path': [2, 3, 5]}\n",
    "    fsm[3] = {'src': 2, 'dst': 3, 'layer': 'maxpooling_layer', 'next_path': [4]}\n",
    "    fsm[4] = {'src': 3, 'dst': 2, 'layer': 'convolutional_layer', 'next_path': [2, 3, 5]}\n",
    "    fsm[5] = {'src': 2, 'dst': 4, 'layer': 'global_maxpooling_layer', 'next_path': [6, 7]}\n",
    "    fsm[6] = {'src': 4, 'dst': 5, 'layer': 'dense_layer', 'next_path': [8, 9, 11]}\n",
    "    fsm[7] = {'src': 4, 'dst': 6, 'layer': 'dropout_layer', 'next_path': [10, 12]}\n",
    "    fsm[8] = {'src': 5, 'dst': 5, 'layer': 'dense_layer', 'next_path': [8, 9, 11]}\n",
    "    fsm[9] = {'src': 5, 'dst': 6, 'layer': 'dropout_layer', 'next_path': [10, 12]}\n",
    "    fsm[10] = {'src': 6, 'dst': 5, 'layer': 'dense_layer', 'next_path': [8, 9, 11]}\n",
    "    fsm[11] = {'src': 5, 'dst': 7, 'layer': 'output_layer', 'next_path': []}\n",
    "    fsm[12] = {'src': 6, 'dst': 7, 'layer': 'output_layer', 'next_path': []}\n",
    "\n",
    "    return fsm\n",
    "\n",
    "def mutateFSM():\n",
    "    fsm = {}\n",
    "    fsm['convolutional_layer'] = {'before': ['convolutional_layer'],\n",
    "                                  'after': ['convolutional_layer'], 'change': 'maxpooling_layer'}\n",
    "    fsm['maxpooling_layer'] = {'before': ['convolutional_layer'],\n",
    "                               'after': ['convolutional_layer'], 'change': 'convolutional_layer'}\n",
    "    fsm['dense_layer'] = {'before': ['global_maxpooling_layer', 'dense_layer'],\n",
    "                          'after': ['dense_layer'], 'change': 'dropout_layer'}\n",
    "    fsm['dropout_layer'] = {'before': ['global_maxpooling_layer', 'dense_layer'],\n",
    "                            'after': ['dense_layer'], 'change': 'dense_layer'}\n",
    "\n",
    "    return fsm\n",
    "\n",
    "def addFSM():\n",
    "    fsm = {}\n",
    "    fsm['convolutional_layer'] = {'before': ['convolutional_layer'],\n",
    "                                  'add': ['convolutional_layer', 'maxpooling_layer']}\n",
    "    fsm['maxpooling_layer'] = {'before': ['convolutional_layer'],\n",
    "                               'add': ['convolutional_layer']}\n",
    "    fsm['dense_layer'] = {'before': ['global_maxpooling_layer', 'dense_layer'],\n",
    "                          'add': ['dense_layer', 'dropout_layer']}\n",
    "    fsm['dropout_layer'] = {'before': ['global_maxpooling_layer', 'dense_layer'],\n",
    "                            'add': ['dense_layer']}\n",
    "\n",
    "    return fsm\n",
    "\n",
    "def addConvLayer(idx, toolbox, toolboxes, defaultVal, layerparameters):\n",
    "    toolbox.register('num_filters' + str(idx), layerparameters['num_filters'][0],\n",
    "                     layerparameters['num_filters'][1], layerparameters['num_filters'][2])\n",
    "    toolboxes.append(toolbox.__getattribute__('num_filters' + str(idx)))\n",
    "    toolbox.register('kernel_size' + str(idx), layerparameters['kernel_size'][0],\n",
    "                     layerparameters['kernel_size'][1], layerparameters['kernel_size'][2])\n",
    "    toolboxes.append(toolbox.__getattribute__('kernel_size' + str(idx)))\n",
    "    toolbox.register('conv_activation_func' + str(idx), layerparameters['conv_activation_func'][0],\n",
    "                     layerparameters['conv_activation_func'][1])\n",
    "    toolboxes.append(toolbox.__getattribute__('conv_activation_func' + str(idx)))\n",
    "    toolbox.register('conv_init_mode' + str(idx), layerparameters['conv_init_mode'][0],\n",
    "                     layerparameters['conv_init_mode'][1])\n",
    "    toolboxes.append(toolbox.__getattribute__('conv_init_mode' + str(idx)))\n",
    "    toolbox.register('conv_weight_constraint' + str(idx), layerparameters['conv_weight_constraint'][0],\n",
    "                     layerparameters['conv_weight_constraint'][1], layerparameters['conv_weight_constraint'][2])\n",
    "    toolboxes.append(toolbox.__getattribute__('conv_weight_constraint' + str(idx)))\n",
    "\n",
    "    defaultVal.update({'num_filters' + str(idx): 64})\n",
    "    defaultVal.update({'kernel_size' + str(idx): 3})\n",
    "    defaultVal.update({'conv_activation_func' + str(idx): \"relu\"})\n",
    "    defaultVal.update({'conv_init_mode' + str(idx): \"glorot_uniform\"})\n",
    "    defaultVal.update({'conv_weight_constraint' + str(idx): 3})\n",
    "\n",
    "\n",
    "def addDenseLayer(idx, toolbox, toolboxes, defaultVal, layerparameters):\n",
    "    toolbox.register('neurons' + str(idx), layerparameters['neurons'][0],\n",
    "                     layerparameters['neurons'][1], layerparameters['neurons'][2])\n",
    "    toolboxes.append(toolbox.__getattribute__('neurons' + str(idx)))\n",
    "    toolbox.register('dense_activation_func' + str(idx), layerparameters['dense_activation_func'][0],\n",
    "                     layerparameters['dense_activation_func'][1])\n",
    "    toolboxes.append(toolbox.__getattribute__('dense_activation_func' + str(idx)))\n",
    "    toolbox.register('dense_init_mode' + str(idx), layerparameters['dense_init_mode'][0],\n",
    "                     layerparameters['dense_init_mode'][1])\n",
    "    toolboxes.append(toolbox.__getattribute__('dense_init_mode' + str(idx)))\n",
    "    toolbox.register('dense_weight_constraint' + str(idx), layerparameters['dense_weight_constraint'][0],\n",
    "                     layerparameters['dense_weight_constraint'][1], layerparameters['dense_weight_constraint'][2])\n",
    "    toolboxes.append(toolbox.__getattribute__('dense_weight_constraint' + str(idx)))\n",
    "\n",
    "    defaultVal.update({'neurons' + str(idx): 1})\n",
    "    defaultVal.update({'dense_activation_func' + str(idx): \"relu\"})\n",
    "    defaultVal.update({'dense_init_mode' + str(idx): \"glorot_uniform\"})\n",
    "    defaultVal.update({'dense_weight_constraint' + str(idx): 3})\n",
    "\n",
    "\n",
    "def addMaxPoolingLayer(idx, toolbox, toolboxes, defaultVal, layerparameters):\n",
    "    toolbox.register('pool_size' + str(idx), layerparameters['pool_size'][0],\n",
    "                     layerparameters['pool_size'][1], layerparameters['pool_size'][2])\n",
    "    toolboxes.append(toolbox.__getattribute__('pool_size' + str(idx)))\n",
    "\n",
    "    defaultVal.update({'pool_size' + str(idx): 5})\n",
    "\n",
    "\n",
    "def addDropoutLayer(idx, toolbox, toolboxes, defaultVal, layerparameters):\n",
    "    toolbox.register('dropout_rate' + str(idx), layerparameters['dropout_rate'][0],\n",
    "                     layerparameters['dropout_rate'][1], layerparameters['dropout_rate'][2])\n",
    "    toolboxes.append(toolbox.__getattribute__('dropout_rate' + str(idx)))\n",
    "\n",
    "    defaultVal.update({'dropout_rate' + str(idx): 0.2})\n",
    "\n",
    "def getLayerSize(layer, conv_idx, dense_idx, dropout_idx, maxpooling_idx):\n",
    "    if layer == 'convolutional_layer':\n",
    "        conv_idx += 1\n",
    "    elif layer == 'dense_layer':\n",
    "        dense_idx += 1\n",
    "    elif layer == 'dropout_layer':\n",
    "        dropout_idx += 1\n",
    "    elif layer == 'maxpooling_layer':\n",
    "        maxpooling_idx += 1\n",
    "    return conv_idx, dense_idx, dropout_idx, maxpooling_idx\n",
    "\n",
    "\n",
    "def getMaxLayerSize(conv_idx, dense_idx, dropout_idx, maxpooling_idx, max_conv_idx, max_dense_idx, max_dropout_idx,\n",
    "                    max_maxpooling_idx):\n",
    "    if conv_idx > max_conv_idx:\n",
    "        max_conv_idx = conv_idx\n",
    "    if dense_idx > max_dense_idx:\n",
    "        max_dense_idx = dense_idx\n",
    "    if dropout_idx > max_dropout_idx:\n",
    "        max_dropout_idx = dropout_idx\n",
    "    if maxpooling_idx > max_maxpooling_idx:\n",
    "        max_maxpooling_idx = maxpooling_idx\n",
    "\n",
    "    return max_conv_idx, max_dense_idx, max_dropout_idx, max_maxpooling_idx\n",
    "\n",
    "\n",
    "def addLayerToolboxes(max_conv_idx, max_dense_idx, max_dropout_idx, max_maxpooling_idx, toolbox, toolboxes, defaultVal,\n",
    "                      layerparameters):\n",
    "    idx = 0\n",
    "    while idx < max_conv_idx:\n",
    "        idx += 1\n",
    "        addConvLayer(idx, toolbox, toolboxes, defaultVal, layerparameters)\n",
    "\n",
    "    idx = 0\n",
    "    while idx < max_dense_idx:\n",
    "        idx += 1\n",
    "        addDenseLayer(idx, toolbox, toolboxes, defaultVal, layerparameters)\n",
    "\n",
    "    idx = 0\n",
    "    while idx < max_maxpooling_idx:\n",
    "        idx += 1\n",
    "        addMaxPoolingLayer(idx, toolbox, toolboxes, defaultVal, layerparameters)\n",
    "\n",
    "    idx = 0\n",
    "    while idx < max_dropout_idx:\n",
    "        idx += 1\n",
    "        addDropoutLayer(idx, toolbox, toolboxes, defaultVal, layerparameters)\n",
    "\n",
    "\n",
    "def generateFSM(n_pop, layerparameters, toolbox, toolboxes, defaultVal):\n",
    "    fsm = FSM()\n",
    "\n",
    "    path_ind = {}\n",
    "    max_conv_idx = 0\n",
    "    max_dense_idx = 0\n",
    "    max_dropout_idx = 0\n",
    "    max_maxpooling_idx = 0\n",
    "\n",
    "    for ind in range(0, n_pop):\n",
    "        idx = conv_idx = dense_idx = dropout_idx = maxpooling_idx = 0\n",
    "        path = [fsm[idx]['layer']]\n",
    "        while len(fsm[idx]['next_path']) != 0:\n",
    "            idx = random.choice(fsm[idx]['next_path'])\n",
    "            layer = fsm[idx]['layer']\n",
    "            path.append(layer)\n",
    "            conv_idx, dense_idx, dropout_idx, maxpooling_idx = getLayerSize(layer, conv_idx, dense_idx, dropout_idx,\n",
    "                                                                            maxpooling_idx)\n",
    "\n",
    "        max_conv_idx, max_dense_idx, max_dropout_idx, max_maxpooling_idx = getMaxLayerSize(conv_idx, dense_idx,\n",
    "                                                                                           dropout_idx, maxpooling_idx,\n",
    "                                                                                           max_conv_idx, max_dense_idx,\n",
    "                                                                                           max_dropout_idx,\n",
    "                                                                                           max_maxpooling_idx)\n",
    "\n",
    "        path_ind[ind] = path\n",
    "\n",
    "    addLayerToolboxes(max_conv_idx, max_dense_idx, max_dropout_idx, max_maxpooling_idx, toolbox, toolboxes, defaultVal,\n",
    "                      layerparameters)\n",
    "\n",
    "    return path_ind, max_conv_idx, max_maxpooling_idx, max_dense_idx, max_dropout_idx\n",
    "\n",
    "\n",
    "def openFSM(df, layerparameters, toolbox, toolboxes, defaultVal):\n",
    "    path_ind = {}\n",
    "    fitnesses = []\n",
    "\n",
    "    hyperparams = [s for s in list(df.columns) if not 'Unnamed' in s]\n",
    "\n",
    "    max_conv_idx = sum('num_filters' in s for s in hyperparams)\n",
    "    max_dense_idx = sum('neurons' in s for s in hyperparams)\n",
    "    max_dropout_idx = sum('dropout_rate' in s for s in hyperparams)\n",
    "    max_maxpooling_idx = sum('pool_size' in s for s in hyperparams)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        path = [s for s in row if 'layer' in str(s)]\n",
    "        fitness = [s for s in row if str(s).replace('.', '', 1).isdigit()]\n",
    "        fitnesses.append(tuple([float(fitness[0])]))\n",
    "        path_ind[index] = path\n",
    "\n",
    "    addLayerToolboxes(max_conv_idx, max_dense_idx, max_dropout_idx, max_maxpooling_idx, toolbox, toolboxes, defaultVal,\n",
    "                      layerparameters)\n",
    "\n",
    "    return path_ind, fitnesses, max_conv_idx, max_maxpooling_idx, max_dense_idx, max_dropout_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-QjUHS9ZWs7U"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DObu8fBWr48"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def cnn_model(self, vocab_size, maxlen, embedding_matrix, indiv, path):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        conv_idx = dense_idx = dropout_idx = maxpooling_idx = 0\n",
    "        for layer in path:\n",
    "            if layer == 'embedding_layer':\n",
    "                model.add(\n",
    "                    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=indiv['output_dim'],\n",
    "                                     weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "            elif layer == 'convolutional_layer':\n",
    "                conv_idx += 1\n",
    "                model.add(tf.keras.layers.Conv1D(indiv['num_filters' + str(conv_idx)], indiv['kernel_size' + str(conv_idx)],\n",
    "                                        kernel_initializer=indiv['conv_init_mode' + str(conv_idx)],\n",
    "                                        activation=indiv['conv_activation_func' + str(conv_idx)],\n",
    "                                        kernel_constraint=tf.keras.constraints.max_norm(indiv['conv_weight_constraint' + str(conv_idx)]),\n",
    "                                        data_format='channels_first'))\n",
    "            elif layer == 'dense_layer':\n",
    "                dense_idx += 1\n",
    "                model.add(tf.keras.layers.Dense(indiv['neurons' + str(dense_idx)],\n",
    "                                       kernel_initializer=indiv['dense_init_mode' + str(dense_idx)],\n",
    "                                       activation=indiv['dense_activation_func' + str(dense_idx)],\n",
    "                                       kernel_constraint=tf.keras.constraints.max_norm(indiv['dense_weight_constraint' + str(dense_idx)])))\n",
    "            elif layer == 'dropout_layer':\n",
    "                dropout_idx += 1\n",
    "                model.add(tf.keras.layers.Dropout(indiv['dropout_rate' + str(dropout_idx)]))\n",
    "            elif layer == 'maxpooling_layer':\n",
    "                maxpooling_idx += 1\n",
    "                model.add(tf.keras.layers.MaxPooling1D(indiv['pool_size' + str(maxpooling_idx)]))\n",
    "            elif layer == 'global_maxpooling_layer':\n",
    "                model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "            elif layer == 'output_layer':\n",
    "                model.add(tf.keras.layers.Dense(1, kernel_initializer=indiv['output_init_mode'], activation='sigmoid'))\n",
    "\n",
    "        if indiv['optimizer'] == 'sgd':\n",
    "            opt = tf.keras.optimizers.SGD(lr=indiv['learning_rate'], momentum=indiv['momentum'], decay=0.0,\n",
    "                                 nesterov=False)\n",
    "        elif indiv['optimizer'] == 'rmsprop':\n",
    "            opt = tf.keras.optimizers.RMSprop(lr=indiv['learning_rate'], rho=0.9, epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adagrad':\n",
    "            opt = tf.keras.optimizers.Adagrad(lr=indiv['learning_rate'], epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adadelta':\n",
    "            opt = tf.keras.optimizers.Adadelta(lr=indiv['learning_rate'], rho=0.95, epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adam':\n",
    "            opt = tf.keras.optimizers.Adam(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                  decay=0.0, amsgrad=False)\n",
    "        elif indiv['optimizer'] == 'adamax':\n",
    "            opt = tf.keras.optimizers.Adamax(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                    decay=0.0)\n",
    "        elif indiv['optimizer'] == 'nadam':\n",
    "            opt = tf.keras.optimizers.Nadam(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                   schedule_decay=0.004)\n",
    "\n",
    "        util = utility()\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[util.f1_m])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsfyjDRoWVMU"
   },
   "source": [
    "# Fitness Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hn9qqGyhWS8x"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "from time import sleep\n",
    "import gc\n",
    "\n",
    "util = utility()\n",
    "cnn = CNN()\n",
    "\n",
    "def FitnessCalculation(individual, cfold, defaultVal, resultsPath, testing_name):\n",
    "    indiv = collections.OrderedDict()\n",
    "    i = 0\n",
    "    for key in defaultVal.keys():\n",
    "        indiv[key] = individual[i]\n",
    "        i += 1\n",
    "\n",
    "    path = individual[len(defaultVal):len(individual)]\n",
    "\n",
    "    return crossfold(indiv, path, cfold, resultsPath, testing_name)\n",
    "\n",
    "\n",
    "def crossfold(indiv, path, fold, resultsPath, testing_name):\n",
    "    if indiv['output_dim'] == 50:\n",
    "        embedding_mtx = fold['embedding_matrix'][0]\n",
    "    elif indiv['output_dim'] == 100:\n",
    "        embedding_mtx = fold['embedding_matrix'][1]\n",
    "    elif indiv['output_dim'] == 200:\n",
    "        embedding_mtx = fold['embedding_matrix'][2]\n",
    "    elif indiv['output_dim'] == 300:\n",
    "        embedding_mtx = fold['embedding_matrix'][3]\n",
    "\n",
    "    model = cnn.cnn_model(fold['vocab_size'], fold['maxlen'], embedding_mtx,\n",
    "                          indiv, path)\n",
    "    \n",
    "    #early stopping\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_f1_m', mode='max', verbose=True, patience=10)]\n",
    "\n",
    "    #save the best model\n",
    "    callbacks += [tf.keras.callbacks.ModelCheckpoint(resultsPath + testing_name + \".h5\", monitor='val_f1_m', mode='max', verbose=False, \n",
    "                                  save_best_only=True)]\n",
    "\n",
    "    class_weight = {0: 0.25,\n",
    "                    1: 0.75}\n",
    "    model.fit(fold['X_train'], fold['y_train'], epochs=indiv['epochs'], verbose=False, \n",
    "              validation_data=(fold['X_val'], fold['y_val']), use_multiprocessing=False,\n",
    "              batch_size=indiv['batch_size'], callbacks=callbacks, class_weight=class_weight)\n",
    "    \n",
    "    dependencies = {\n",
    "    'f1_m': util.f1_m\n",
    "    }\n",
    "\n",
    "    # load the saved model\n",
    "    for x in range(0, 4):  # try 4 times\n",
    "        try:\n",
    "            # msg.send()\n",
    "            saved_model = tf.keras.models.load_model(resultsPath + testing_name + \".h5\", custom_objects=dependencies)\n",
    "            str_error = None\n",
    "        except Exception as e:\n",
    "            print('An error occurs when loading saved model.')\n",
    "            str_error = e\n",
    "            pass\n",
    "\n",
    "        if str_error:\n",
    "            sleep(2)  # wait for 2 seconds before trying to fetch the data again\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "\n",
    "    y_pred = saved_model.predict_classes(fold['X_val'])\n",
    "\n",
    "    os.remove(resultsPath + testing_name + \".h5\")\n",
    "\n",
    "    # CNN metrics\n",
    "    accuracyScore, precisionScoreBinary, recallScoreBinary, f1ScoreBinary = util.get_testing_metric(fold['y_val'],\n",
    "                                                                                                    y_pred)\n",
    "    \n",
    "    del embedding_mtx, indiv, path, fold, resultsPath, testing_name, model, callbacks, saved_model, accuracyScore, precisionScoreBinary, recallScoreBinary\n",
    "    gc.collect()\n",
    "    \n",
    "    return f1ScoreBinary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EJ38f_Y7WJSv"
   },
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mE6C_3bWHcU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from operator import attrgetter\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "import itertools\n",
    "\n",
    "\n",
    "class GeneticAlgorithm:\n",
    "    __slots__ = (\n",
    "        \"toolbox\", \"toolboxes\", \"cross_rate\", \"mut_rate\", \"n_pop\", \"n_gen\", \"resultsPath\", \"testing_name\", \"cfold\",\n",
    "        \"globalparameters\", \"layerparameters\", \"defaultVal\", \"path_ind\", \"max_conv_idx\", \"max_maxpooling_idx\",\n",
    "        \"max_dense_idx\", \"max_dropout_idx\")\n",
    "\n",
    "    def __init__(self, toolbox, toolboxes, cross_rate, mut_rate, n_pop, n_gen, resultsPath, testing_name,\n",
    "                 cfold, globalparameters, layerparameters, defaultVal, path_ind, max_conv_idx, max_maxpooling_idx,\n",
    "                 max_dense_idx, max_dropout_idx):\n",
    "        self.toolbox = toolbox\n",
    "        self.toolboxes = toolboxes\n",
    "        self.cross_rate = cross_rate\n",
    "        self.mut_rate = mut_rate\n",
    "        self.n_pop = n_pop\n",
    "        self.n_gen = n_gen\n",
    "        self.resultsPath = resultsPath\n",
    "        self.testing_name = testing_name\n",
    "        self.cfold = cfold\n",
    "        self.globalparameters = globalparameters\n",
    "        self.layerparameters = layerparameters\n",
    "        self.defaultVal = defaultVal\n",
    "        self.path_ind = path_ind\n",
    "        self.max_conv_idx = max_conv_idx\n",
    "        self.max_maxpooling_idx = max_maxpooling_idx\n",
    "        self.max_dense_idx = max_dense_idx\n",
    "        self.max_dropout_idx = max_dropout_idx\n",
    "\n",
    "    def fitnessCalc(self, individual):\n",
    "        i = 0\n",
    "        if len(individual.fitness.values) == 0:\n",
    "            if (0 in individual or '' in individual or 'False' in individual or None in individual):\n",
    "                for param in self.defaultVal:\n",
    "                    if individual[i] == 0 or individual[i] == '' or individual[i] == 'False' or individual[i] == None:\n",
    "                        individual[i] = self.defaultVal[param]\n",
    "                    i += 1\n",
    "\n",
    "            fc = FitnessCalculation(individual, self.cfold, self.defaultVal, self.resultsPath, self.testing_name)\n",
    "        else:\n",
    "            fc = individual.fitness.values[0]\n",
    "        print('{} {}'.format(datetime.datetime.now(), fc))\n",
    "        return fc,\n",
    "\n",
    "    def write_result(self):\n",
    "        # Create Testing Results\n",
    "        f = open(self.resultsPath + self.testing_name + \".csv\", \"a+\")\n",
    "        text = \"i,min,max,mean,std,avgdistance,time,CR,MR\"\n",
    "        for param in self.defaultVal:\n",
    "            text += \",{0}\".format(param)\n",
    "        text += \"\\n\"       \n",
    "        f.write(text)\n",
    "        f.close()\n",
    "\n",
    "        # Create Last Population file\n",
    "        f = open(self.resultsPath + self.testing_name + \"lastpop.csv\", 'a+')\n",
    "        text = \"i,f1score\"\n",
    "        for param in self.defaultVal:\n",
    "            text += \",{0}\".format(param)\n",
    "        text += \"\\n\"\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "\n",
    "    def std_calc(self, fits, length):\n",
    "        mean = sum(fits) / length\n",
    "        sum2 = sum(x * x for x in fits)\n",
    "        std = abs(sum2 / length - mean ** 2) ** 0.5\n",
    "\n",
    "        return mean, std\n",
    "    \n",
    "    def distance_calc(self, pop):\n",
    "        distances = []\n",
    "        for subset in itertools.combinations(pop, 2):\n",
    "            distances.append(distance.hamming(subset[0][0:subset[0].index('embedding_layer')],\n",
    "                                              subset[1][0:subset[1].index('embedding_layer')]))\n",
    "\n",
    "        avgDistance = sum(distances) / len(distances)\n",
    "        \n",
    "        return avgDistance\n",
    "\n",
    "\n",
    "    def invalid_fitness_calc(self, pop):\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in pop if not ind.fitness.valid]\n",
    "        fitnesses = map(self.toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "    def mutHyperparam(self, individual, indpb):\n",
    "        toolboxesSize = len(self.toolboxes)\n",
    "        fsm = FSM()\n",
    "        mutatefsm = mutateFSM()\n",
    "        addfsm = addFSM()\n",
    "\n",
    "        # Mutation for the Hyperparameter Chromosomes\n",
    "        for i in range(toolboxesSize):\n",
    "            if random.random() < indpb:\n",
    "                if len(self.toolboxes[i].args) == 1:\n",
    "                    individual[i] = self.toolboxes[i].func(self.toolboxes[i].args[0])\n",
    "                else:\n",
    "                    individual[i] = self.toolboxes[i].func(self.toolboxes[i].args[0], self.toolboxes[i].args[1])\n",
    "\n",
    "        # Mutation for the Architecture Chromosomes\n",
    "        archChrom = individual[individual.index('convolutional_layer'):individual.index('output_layer')]\n",
    "        size = len(archChrom)\n",
    "\n",
    "        for i in range(1, size):\n",
    "            if random.random() < indpb:\n",
    "                if (i>=size):\n",
    "                    break\n",
    "                \n",
    "                if (archChrom[i] == 'global_maxpooling_layer'):\n",
    "                    continue\n",
    "\n",
    "                selectMutType = random.randint(0, 2)\n",
    "                # Remove the layer\n",
    "                if selectMutType == 0:\n",
    "                    for key in fsm:\n",
    "                        if fsm[key]['layer'] in archChrom[i - 1:i]:\n",
    "                            for j in fsm[key]['next_path']:\n",
    "                                if i != size - 1 and i + 1 < size and fsm[j]['layer'] == archChrom[i + 1]:\n",
    "                                    archChrom.remove(archChrom[i])\n",
    "#                                   print('individual before remove', individual)\n",
    "                                    individual[\n",
    "                                    individual.index('convolutional_layer'):individual.index('output_layer')] = archChrom\n",
    "#                                   print('individual after remove', individual)\n",
    "                                    size -= 1          \n",
    "                                    break\n",
    "                            else:\n",
    "                                continue\n",
    "                            break                              \n",
    "\n",
    "                # Change the layer\n",
    "                elif selectMutType == 1:\n",
    "                    if i == size - 1:\n",
    "                        if (archChrom[i] == 'dropout_layer') or (archChrom[i] == 'dense_layer' and 'dropout_layer' not in archChrom[i - 1:i]):\n",
    "                            if (mutatefsm[archChrom[i]]['change'] == 'convolutional_layer' and individual.count(\n",
    "                                    'convolutional_layer') < self.max_conv_idx) or (\n",
    "                                    mutatefsm[archChrom[i]]['change'] == 'dense_layer' and individual.count('dense_layer')\n",
    "                                    < self.max_dense_idx) or (\n",
    "                                    mutatefsm[archChrom[i]]['change'] == 'maxpooling_layer' and individual.count(\n",
    "                                'maxpooling_layer') < self.max_maxpooling_idx) or (\n",
    "                                    mutatefsm[archChrom[i]]['change'] == 'dropout_layer' and individual.count('dropout_layer')\n",
    "                                    < self.max_dropout_idx):\n",
    "                                archChrom[i] = mutatefsm[archChrom[i]]['change']\n",
    "#                                 print('individual before change', individual)\n",
    "                                individual[\n",
    "                                individual.index('convolutional_layer'):individual.index('output_layer')] = archChrom\n",
    "#                                 print('individual after change', individual)\n",
    "\n",
    "                    else:\n",
    "                        if all(item in mutatefsm[archChrom[i]]['before'] for item in archChrom[i - 1:i]) and archChrom[i + 1] in mutatefsm[archChrom[i]]['after']:\n",
    "                            if (mutatefsm[archChrom[i]]['change'] == 'convolutional_layer' and individual.count(\n",
    "                                    'convolutional_layer') < self.max_conv_idx) or (\n",
    "                                    mutatefsm[archChrom[i]]['change'] == 'dense_layer' and individual.count('dense_layer')\n",
    "                                    < self.max_dense_idx) or (\n",
    "                                    mutatefsm[archChrom[i]]['change'] == 'maxpooling_layer' and individual.count(\n",
    "                                'maxpooling_layer') < self.max_maxpooling_idx) or (\n",
    "                                    mutatefsm[archChrom[i]]['change'] == 'dropout_layer' and individual.count('dropout_layer')\n",
    "                                    < self.max_dropout_idx):\n",
    "                                \n",
    "                                archChrom[i] = mutatefsm[archChrom[i]]['change']\n",
    "#                                 print('individual before change', individual)\n",
    "                                individual[\n",
    "                                individual.index('convolutional_layer'):individual.index('output_layer')] = archChrom\n",
    "#                                 print('individual after change', individual)\n",
    "                # Add a layer\n",
    "                elif selectMutType == 2:\n",
    "                    for key in addfsm:\n",
    "                        if key in archChrom[i] and all(item in addfsm[archChrom[i]]['before'] for item in archChrom[i - 1:i]):\n",
    "                            for layer in addfsm[archChrom[i]]['add']:\n",
    "                                if (layer == 'convolutional_layer' and individual.count('convolutional_layer') < self.max_conv_idx) or (\n",
    "                                layer == 'dense_layer' and individual.count('dense_layer') < self.max_dense_idx) or (\n",
    "                                layer == 'maxpooling_layer' and individual.count('maxpooling_layer') < self.max_maxpooling_idx) or (\n",
    "                                layer == 'dropout_layer' and individual.count('dropout_layer') < self.max_dropout_idx):\n",
    "#                                     print('individual before add', individual)\n",
    "                                    archChrom.insert(i, layer)\n",
    "                                    individual[individual.index('convolutional_layer'):individual.index('output_layer')] = archChrom\n",
    "#                                     print('individual after add', individual)\n",
    "                                    break\n",
    "                        \n",
    "        return individual,\n",
    "\n",
    "    def cxTwoPoint(self, ind1, ind2, pop, offspring):\n",
    "        # Crossover for hyperparameter chromosomes\n",
    "        size = ind1.index('embedding_layer')\n",
    "        selectCxType = random.randint(0, 2)\n",
    "        # One point crossover\n",
    "        if selectCxType == 0:\n",
    "#             print('ind1 before one-point crossover:', ind1)\n",
    "#             print('ind2 before one-point crossover:', ind2)\n",
    "            cxpoint = random.randint(1, size - 1)\n",
    "            ind1[cxpoint:], ind2[cxpoint:] = ind2[cxpoint:], ind1[cxpoint:]\n",
    "#             print('ind1 after one-point crossover:', ind1)\n",
    "#             print('ind2 after one-point crossover:', ind2)\n",
    "        # Two-point crossover\n",
    "        elif selectCxType == 1:\n",
    "#             print('ind1 before two-point crossover:', ind1)\n",
    "#             print('ind2 before two-point crossover:', ind2)\n",
    "            cxpoint1 = random.randint(1, size - 1)\n",
    "            cxpoint2 = random.randint(1, size - 1)\n",
    "            if cxpoint2 >= cxpoint1:\n",
    "                cxpoint2 += 1\n",
    "            else:  # Swap the two cx points\n",
    "                cxpoint1, cxpoint2 = cxpoint2, cxpoint1\n",
    "\n",
    "            ind1[cxpoint1:cxpoint2], ind2[cxpoint1:cxpoint2] \\\n",
    "                = ind2[cxpoint1:cxpoint2], ind1[cxpoint1:cxpoint2]\n",
    "#             print('ind1 after two-point crossover:', ind1)\n",
    "#             print('ind2 after two-point crossover:', ind2)\n",
    "        # Uniform crossover\n",
    "        elif selectCxType == 2:\n",
    "#             print('ind1 before uniform crossover:', ind1)\n",
    "#             print('ind2 before uniform crossover:', ind2)\n",
    "            for i in range(size):\n",
    "                if random.random() < self.cross_rate:\n",
    "                    ind1[i], ind2[i] = ind2[i], ind1[i]\n",
    "#             print('ind1 after uniform crossover:', ind1)\n",
    "#             print('ind2 after uniform crossover:', ind2)\n",
    "\n",
    "        # Crossover for architecture chromosomes\n",
    "        # One-cut point crossover from the Global MaxPooling layer\n",
    "        cxpoint1 = ind1.index('global_maxpooling_layer')\n",
    "        cxpoint2 = ind2.index('global_maxpooling_layer')\n",
    "        ind1[cxpoint1:], ind2[cxpoint2:] = ind2[cxpoint2:], ind1[cxpoint1:]\n",
    "\n",
    "        max_drop_layer = max(ind1.count('dropout_layer'), ind2.count('dropout_layer'))\n",
    "        if max_drop_layer > self.max_dropout_idx:\n",
    "            idx = self.max_dropout_idx\n",
    "            self.max_dropout_idx = max_drop_layer\n",
    "\n",
    "            while idx < self.max_dropout_idx:\n",
    "                idx += 1\n",
    "                addDropoutLayer(idx, self.toolbox, self.toolboxes, self.defaultVal, self.layerparameters)\n",
    "\n",
    "                for ind in pop + offspring:\n",
    "                    ind.insert(size, random.uniform(0, 1))\n",
    "                size += 1\n",
    "\n",
    "                self.write_result()\n",
    "\n",
    "        return ind1, ind2\n",
    "\n",
    "    def runGA(self, lastPop=[], lastFitnesses=[]):\n",
    "        creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "        self.toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                              self.toolboxes, n=1)\n",
    "        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n",
    "        self.toolbox.register(\"evaluate\", self.fitnessCalc)\n",
    "        self.toolbox.register(\"mate\", self.cxTwoPoint)\n",
    "        self.toolbox.register(\"mutate\", self.mutHyperparam, indpb=self.mut_rate)\n",
    "        self.toolbox.register(\"select\", tools.selBest)\n",
    "\n",
    "        pop = self.toolbox.population(n=self.n_pop)\n",
    "\n",
    "        idx = 0\n",
    "        for ind in pop:\n",
    "            if lastPop:\n",
    "                ind[:] = lastPop[idx]\n",
    "            ind.extend(self.path_ind[idx])\n",
    "            idx += 1\n",
    "        \n",
    "        if lastFitnesses:\n",
    "            # Fitnesses from previous population\n",
    "            fitnesses = lastFitnesses\n",
    "        else:\n",
    "            # Evaluate the entire population\n",
    "            fitnesses = list(map(self.toolbox.evaluate, pop))\n",
    "\n",
    "        for ind, fit in zip(pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        self.write_result()\n",
    "        \n",
    "        g = 90\n",
    "        while g < self.n_gen:\n",
    "            then = time.time()\n",
    "            g = g + 1\n",
    "            print('{} {}'.format(datetime.datetime.now(), \"-- Generation %i --\" % g))\n",
    "            \n",
    "            # Select the next generation individuals\n",
    "            offspring = self.toolbox.select(pop, len(pop))\n",
    "            # Clone the selected individuals\n",
    "            offspring = list(map(self.toolbox.clone, offspring))\n",
    "\n",
    "            # Apply crossover and mutation on the offspring\n",
    "            for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "                if random.random() < self.cross_rate:\n",
    "                    self.toolbox.mate(child1, child2, pop, offspring)\n",
    "                    del child1.fitness.values\n",
    "                    del child2.fitness.values\n",
    "\n",
    "            for mutant in offspring:\n",
    "                if random.random() < self.mut_rate:\n",
    "                    self.toolbox.mutate(mutant)\n",
    "                    del mutant.fitness.values\n",
    "\n",
    "            # Evaluate the individuals with an invalid fitness\n",
    "            self.invalid_fitness_calc(offspring)\n",
    "\n",
    "            pop[:] = self.toolbox.select(pop + offspring, self.n_pop)\n",
    "\n",
    "            # Gather all the fitnesses in one list and print the stats\n",
    "            fits = [ind.fitness.values[0] for ind in pop]\n",
    "\n",
    "            length = len(pop)\n",
    "            mean, std = self.std_calc(fits, length)\n",
    "            avgDistance = self.distance_calc(pop)\n",
    "            best = max(pop, key=attrgetter(\"fitness\"))\n",
    "            print('{} {}'.format(datetime.datetime.now(), \"  Min %s\" % min(fits)))\n",
    "            print('{} {}'.format(datetime.datetime.now(), \"  Max %s\" % max(fits)))\n",
    "            print('{} {}'.format(datetime.datetime.now(), \"  Avg %s\" % mean))\n",
    "            print('{} {}'.format(datetime.datetime.now(), \"  Std %s\" % std))\n",
    "            print('{} {}'.format(datetime.datetime.now(), \"  Avg Distance %s\" % avgDistance))\n",
    "            print('{} {}'.format(datetime.datetime.now(), best))\n",
    "\n",
    "            now = time.time()\n",
    "            diff = now - then\n",
    "\n",
    "            # save testing data\n",
    "            f = open(self.resultsPath + self.testing_name + \".csv\", 'a')\n",
    "            text = \"{0},{1},{2},{3},{4},{5},{6},{7},{8}\".format(g,min(fits), max(fits), mean, std, avgDistance, diff, self.cross_rate, self.mut_rate)\n",
    "            for param in best:\n",
    "                text += \",{0}\".format(param)\n",
    "            text += \"\\n\"\n",
    "            f.write(text)\n",
    "            f.close()\n",
    "\n",
    "            # save last population data\n",
    "            f = open(self.resultsPath + self.testing_name + \"lastpop.csv\", 'a')\n",
    "            for ind in pop:\n",
    "                text = \"{0},{1}\".format(g,ind.fitness.values[0])\n",
    "                for param in ind:\n",
    "                    text += \",{0}\".format(param)\n",
    "                text += \"\\n\"                 \n",
    "                f.write(text)\n",
    "            \n",
    "            del offspring, std, best, then, now, diff, text\n",
    "            gc.collect()\n",
    "            f.close()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUA1C18jfyE6"
   },
   "source": [
    "# Project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "9OZ4ryogfYra",
    "outputId": "37d80a3b-c3dc-4a87-f222-b09dd455b99c"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "# path\n",
    "\n",
    "training_path = 'TrainingData.csv'\n",
    "trial_path = 'TrialData.csv'\n",
    "population_path = 'NewPop.csv'\n",
    "root_path = '/lab/dbms/fatyanosa'\n",
    "datasetPath = root_path + '/Dataset/Suggestion Mining/'\n",
    "resultsPath = root_path + '/Server1/Suggestion Mining/Results/'\n",
    "archPath = root_path + '/Server1/Suggestion Mining/Architecture/'\n",
    "testing_name = \"GA-CNN\"\n",
    "glovePath = [root_path + '/Glove/glove.6B.50d.txt',\n",
    "                 root_path + '/Glove/glove.6B.100d.txt',\n",
    "                 root_path + '/Glove/glove.6B.200d.txt',\n",
    "                 root_path + '/Glove/glove.6B.300d.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8O1VRBE73ivB"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XqVtrSu3kq4"
   },
   "outputs": [],
   "source": [
    "# crossover rate is the probability with which two individuals\n",
    "cross_rate = 0.8\n",
    "\n",
    "# mutation rate is the probability for mutating an individual\n",
    "mut_rate = 0.2\n",
    "\n",
    "# number of population\n",
    "n_pop = 30\n",
    "\n",
    "# number of generation\n",
    "n_gen = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6kUHesFXh17"
   },
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "U4oAqAYAUmVT",
    "outputId": "91c4c09f-7c50-4389-b964-559ef935952d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-17 04:57:55.793921 -- Generation 91 --\n",
      "2020-06-17 04:58:04.013253 0.526615969581749\n",
      "2020-06-17 04:58:09.088558 0.6859666339548576\n",
      "WARNING:tensorflow:Large dropout rate: 0.765278 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.765278 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.765278 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Epoch 00012: early stopping\n",
      "WARNING:tensorflow:Large dropout rate: 0.765278 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "2020-06-17 04:58:21.358679 0.6753497346840328\n",
      "2020-06-17 04:58:26.500336 0.6666666666666666\n",
      "2020-06-17 04:58:31.743549 0.6676238334529793\n",
      "Epoch 00012: early stopping\n",
      "2020-06-17 04:58:42.763294 0.7400990099009902\n",
      "2020-06-17 04:58:47.850582 0.7104591836734694\n",
      "2020-06-17 04:58:53.138845 0.6313328137178489\n",
      "2020-06-17 04:58:58.276001 0.6806178375685102\n",
      "2020-06-17 04:59:03.389288 0.6829985301322881\n",
      "2020-06-17 04:59:08.640109 0.686217008797654\n",
      "2020-06-17 04:59:13.890849 0.5390199637023594\n",
      "2020-06-17 04:59:18.683051 0.7097775478530781\n",
      "2020-06-17 04:59:23.598745 0.6059071729957807\n",
      "2020-06-17 04:59:30.438753 0.6666666666666666\n",
      "2020-06-17 04:59:35.427857 0.6826640548481882\n",
      "WARNING:tensorflow:Large dropout rate: 0.542718 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "2020-06-17 04:59:40.170610 0.6939443535188216\n",
      "2020-06-17 04:59:45.311956 0.7341772151898734\n",
      "2020-06-17 04:59:50.557615 0.7051027170311465\n",
      "2020-06-17 04:59:55.483646 0.7401484865790977\n",
      "2020-06-17 04:59:59.447453 0.6695402298850573\n",
      "2020-06-17 05:00:04.322144 0.6695402298850573\n",
      "2020-06-17 05:00:09.553394 0.7208569628229363\n",
      "2020-06-17 05:00:14.625618 0.6666666666666666\n",
      "2020-06-17 05:00:19.835562 0.6555386949924127\n",
      "2020-06-17 05:00:24.963822 0.6745737583395107\n",
      "2020-06-17 05:00:24.991673   Min 0.742953777\n",
      "2020-06-17 05:00:24.991973   Max 0.745972739\n",
      "2020-06-17 05:00:24.991998   Avg 0.7434232675333335\n",
      "2020-06-17 05:00:24.992015   Std 0.0008642421059863073\n",
      "2020-06-17 05:00:24.992031   Avg Distance 0.02475685234305923\n",
      "2020-06-17 05:00:24.992059 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:00:24.995009 -- Generation 92 --\n",
      "2020-06-17 05:00:30.276364 0.6661721068249259\n",
      "2020-06-17 05:00:35.596331 0.6373268133659333\n",
      "2020-06-17 05:00:41.394369 0.7119386637458927\n",
      "2020-06-17 05:00:46.674187 0.7079646017699115\n",
      "2020-06-17 05:00:51.986784 0.7065073041168659\n",
      "2020-06-17 05:00:57.651144 0.7285714285714286\n",
      "2020-06-17 05:01:03.534694 0.6888999502239921\n",
      "2020-06-17 05:01:08.745619 0.7066431510875956\n",
      "2020-06-17 05:01:13.981006 0.7003508771929825\n",
      "Epoch 00013: early stopping\n",
      "2020-06-17 05:01:27.386932 0.6844262295081968\n",
      "2020-06-17 05:01:32.867848 0.7292464878671776\n",
      "2020-06-17 05:01:38.414097 0.631911532385466\n",
      "2020-06-17 05:01:43.674164 0.7118847539015606\n",
      "2020-06-17 05:01:49.217153 0.7160493827160495\n",
      "2020-06-17 05:01:54.527219 0.6666666666666666\n",
      "2020-06-17 05:02:02.511267 0.7219343696027634\n",
      "2020-06-17 05:02:07.926370 0.7189988623435721\n",
      "2020-06-17 05:02:13.352385 0.7152466367713004\n",
      "2020-06-17 05:02:18.875939 0.7054619703930576\n",
      "2020-06-17 05:02:26.910314 0.7394094993581515\n",
      "2020-06-17 05:02:32.744159 0.6820186183243508\n",
      "2020-06-17 05:02:38.406562 0.6968449931412894\n",
      "2020-06-17 05:02:44.392872 0.6210780370072405\n",
      "2020-06-17 05:02:50.126573 0.6973218797372409\n",
      "2020-06-17 05:02:55.703346 0.6611817501869858\n",
      "2020-06-17 05:03:01.363699 0.7042640990371389\n",
      "2020-06-17 05:03:01.392827   Min 0.742953777\n",
      "2020-06-17 05:03:01.392876   Max 0.745972739\n",
      "2020-06-17 05:03:01.392888   Avg 0.7435126763333335\n",
      "2020-06-17 05:03:01.392898   Std 0.000880270592930426\n",
      "2020-06-17 05:03:01.392911   Avg Distance 0.02475685234305923\n",
      "2020-06-17 05:03:01.392933 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:03:01.397642 -- Generation 93 --\n",
      "2020-06-17 05:03:06.956791 0.7355623100303951\n",
      "2020-06-17 05:03:12.649170 0.6302931596091205\n",
      "2020-06-17 05:03:18.380688 0.6752460257380771\n",
      "2020-06-17 05:03:24.382913 0.713472485768501\n",
      "2020-06-17 05:03:30.181014 0.6427457098283931\n",
      "2020-06-17 05:03:36.098770 0.7174999999999999\n",
      "2020-06-17 05:03:42.017276 0.72875\n",
      "2020-06-17 05:03:47.880293 0.7216157205240175\n",
      "2020-06-17 05:03:53.680250 0.7214561500275786\n",
      "2020-06-17 05:03:59.322935 0.738898756660746\n",
      "2020-06-17 05:04:05.285389 0.6867816091954023\n",
      "2020-06-17 05:04:11.476563 0.6368\n",
      "2020-06-17 05:04:17.464283 0.7159029649595686\n",
      "Epoch 00011: early stopping\n",
      "2020-06-17 05:04:29.480155 0.0\n",
      "2020-06-17 05:04:35.695901 0.7207310108509423\n",
      "2020-06-17 05:04:41.361518 0.6086956521739131\n",
      "2020-06-17 05:04:47.636401 0.7007874015748032\n",
      "2020-06-17 05:04:53.568734 0.7122923588039867\n",
      "2020-06-17 05:04:59.894809 0.7013388259526262\n",
      "2020-06-17 05:05:07.420065 0.6926970354302241\n",
      "Epoch 00027: early stopping\n",
      "2020-06-17 05:05:26.438683 0.7223208999407934\n",
      "2020-06-17 05:05:32.649785 0.7083775185577943\n",
      "2020-06-17 05:05:38.853156 0.7065868263473053\n",
      "2020-06-17 05:05:45.331077 0.6717924074963959\n",
      "2020-06-17 05:05:45.356924   Min 0.742953777\n",
      "2020-06-17 05:05:45.356972   Max 0.745972739\n",
      "2020-06-17 05:05:45.356985   Avg 0.7436467895333335\n",
      "2020-06-17 05:05:45.356996   Std 0.0008870411006773605\n",
      "2020-06-17 05:05:45.357025   Avg Distance 0.02475685234305923\n",
      "2020-06-17 05:05:45.357055 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:05:45.361995 -- Generation 94 --\n",
      "2020-06-17 05:05:51.564892 0.7383918459796149\n",
      "2020-06-17 05:05:57.825427 0.6949806949806949\n",
      "2020-06-17 05:06:04.387571 0.7329842931937174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-17 05:06:10.756797 0.6918809884014119\n",
      "2020-06-17 05:06:17.110343 0.7038183694530443\n",
      "2020-06-17 05:06:23.453619 0.6666666666666666\n",
      "2020-06-17 05:06:29.677688 0.6990553306342779\n",
      "2020-06-17 05:06:36.407000 0.7023728813559322\n",
      "2020-06-17 05:06:42.571229 0.6717654557042703\n",
      "2020-06-17 05:06:49.203332 0.7032520325203252\n",
      "2020-06-17 05:06:55.714812 0.7221006564551422\n",
      "2020-06-17 05:07:02.233864 0.696527428283845\n",
      "2020-06-17 05:07:08.976311 0.7326388888888888\n",
      "2020-06-17 05:07:15.771020 0.7227456258411844\n",
      "2020-06-17 05:07:22.277140 0.7287761852260198\n",
      "2020-06-17 05:07:27.892355 0.6501095690284879\n",
      "2020-06-17 05:07:34.487674 0.7183908045977012\n",
      "2020-06-17 05:07:41.222308 0.6585714285714286\n",
      "2020-06-17 05:07:47.824913 0.7065616797900263\n",
      "2020-06-17 05:07:55.951698 0.7151515151515152\n",
      "2020-06-17 05:08:03.091149 0.7212220403709766\n",
      "2020-06-17 05:08:09.676744 0.6844444444444444\n",
      "2020-06-17 05:08:16.503276 0.727377521613833\n",
      "2020-06-17 05:08:23.305904 0.6789940828402367\n",
      "2020-06-17 05:08:23.329860   Min 0.742953777\n",
      "2020-06-17 05:08:23.329912   Max 0.745972739\n",
      "2020-06-17 05:08:23.329929   Avg 0.7437361983333336\n",
      "2020-06-17 05:08:23.329940   Std 0.0008802466390231802\n",
      "2020-06-17 05:08:23.329962   Avg Distance 0.02475685234305923\n",
      "2020-06-17 05:08:23.329985 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:08:23.333157 -- Generation 95 --\n",
      "2020-06-17 05:08:30.169010 0.6770579639551875\n",
      "2020-06-17 05:08:36.940390 0.6867816091954023\n",
      "2020-06-17 05:08:44.935332 0.7260115606936416\n",
      "2020-06-17 05:08:51.670531 0.4980392156862746\n",
      "2020-06-17 05:08:58.487588 0.7247706422018348\n",
      "2020-06-17 05:09:05.380984 0.7163355408388522\n",
      "2020-06-17 05:09:12.326815 0.705195563339171\n",
      "2020-06-17 05:09:19.537707 0.6904922454484155\n",
      "2020-06-17 05:09:26.495365 0.6875310481867859\n",
      "2020-06-17 05:09:33.496495 0.693795863909273\n",
      "2020-06-17 05:09:40.662068 0.7164021164021163\n",
      "2020-06-17 05:09:47.690205 0.7219251336898397\n",
      "2020-06-17 05:09:54.945981 0.7240729101194218\n",
      "2020-06-17 05:10:01.530558 0.6897263810015489\n",
      "2020-06-17 05:10:08.675057 0.6685796269727404\n",
      "2020-06-17 05:10:15.962180 0.6666666666666666\n",
      "2020-06-17 05:10:23.133652 0.7291011942174733\n",
      "2020-06-17 05:10:30.302858 0.7163588390501319\n",
      "2020-06-17 05:10:37.891978 0.6820186183243508\n",
      "2020-06-17 05:10:45.015440 0.6988950276243094\n",
      "2020-06-17 05:10:51.868667 0.667302192564347\n",
      "2020-06-17 05:10:58.945997 0.7213930348258707\n",
      "2020-06-17 05:11:06.323047 0.6147540983606556\n",
      "2020-06-17 05:11:13.811183 0.6997885835095137\n",
      "2020-06-17 05:11:21.114650 0.6814159292035398\n",
      "2020-06-17 05:11:29.990921 0.693089430894309\n",
      "2020-06-17 05:11:37.529150 0.6407766990291263\n",
      "2020-06-17 05:11:45.325047 0.6901338621715419\n",
      "2020-06-17 05:11:45.352951   Min 0.742953777\n",
      "2020-06-17 05:11:45.353187   Max 0.745972739\n",
      "2020-06-17 05:11:45.353214   Avg 0.7437361983333336\n",
      "2020-06-17 05:11:45.353231   Std 0.0008802466390231802\n",
      "2020-06-17 05:11:45.353248   Avg Distance 0.02475685234305923\n",
      "2020-06-17 05:11:45.353274 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:11:45.356156 -- Generation 96 --\n",
      "Epoch 00012: early stopping\n",
      "2020-06-17 05:11:59.118196 0.6302040816326531\n",
      "2020-06-17 05:12:07.692097 0.5171102661596958\n",
      "2020-06-17 05:12:16.194136 0.7142022209234365\n",
      "2020-06-17 05:12:23.865523 0.7297783103654883\n",
      "2020-06-17 05:12:32.502690 0.715311004784689\n",
      "Epoch 00014: early stopping\n",
      "2020-06-17 05:12:49.368348 0.6470588235294118\n",
      "2020-06-17 05:12:57.511101 0.6543494996150886\n",
      "2020-06-17 05:13:05.764177 0.6833824975417896\n",
      "2020-06-17 05:13:14.221931 0.6836935166994106\n",
      "2020-06-17 05:13:22.154240 0.7221905305191101\n",
      "2020-06-17 05:13:30.710257 0.7226137091607944\n",
      "2020-06-17 05:13:39.378864 0.6917510853835022\n",
      "2020-06-17 05:13:47.700892 0.684593023255814\n",
      "2020-06-17 05:13:56.995136 0.7052341597796143\n",
      "2020-06-17 05:14:04.936077 0.7172322022621425\n",
      "2020-06-17 05:14:14.408996 0.6705035971223021\n",
      "2020-06-17 05:14:22.759183 0.7410029498525075\n",
      "2020-06-17 05:14:35.698167 0.5688568856885687\n",
      "2020-06-17 05:14:45.108330 0.7298524404086265\n",
      "2020-06-17 05:14:53.597437 0.7331378299120236\n",
      "2020-06-17 05:15:02.292691 0.7130162049137482\n",
      "2020-06-17 05:15:11.172385 0.6926762491444216\n",
      "2020-06-17 05:15:19.869902 0.703405572755418\n",
      "2020-06-17 05:15:28.905539 0.7177777777777777\n",
      "2020-06-17 05:15:37.587740 0.648062015503876\n",
      "2020-06-17 05:15:46.032041 0.6783216783216783\n",
      "2020-06-17 05:15:46.066339   Min 0.742953777\n",
      "2020-06-17 05:15:46.066468   Max 0.745972739\n",
      "2020-06-17 05:15:46.066498   Avg 0.7439821668666668\n",
      "2020-06-17 05:15:46.066780   Std 0.0009984962906598694\n",
      "2020-06-17 05:15:46.066809   Avg Distance 0.04597701149425287\n",
      "2020-06-17 05:15:46.066844 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:15:46.070941 -- Generation 97 --\n",
      "2020-06-17 05:15:54.426718 0.7266666666666667\n",
      "2020-06-17 05:16:02.568291 0.6708253358925145\n",
      "2020-06-17 05:16:11.867427 0.6500389711613407\n",
      "2020-06-17 05:16:20.657898 0.7207310108509423\n",
      "2020-06-17 05:16:28.449758 0.6430260047281324\n",
      "2020-06-17 05:16:37.210907 0.7140633108458745\n",
      "2020-06-17 05:16:45.621212 0.6937751004016064\n",
      "2020-06-17 05:16:54.175471 0.7162329615861214\n",
      "2020-06-17 05:17:03.272415 0.7296340023612751\n",
      "2020-06-17 05:17:12.839196 0.668100358422939\n",
      "2020-06-17 05:17:21.819944 0.6408227848101266\n",
      "2020-06-17 05:17:29.914108 0.6666666666666666\n",
      "2020-06-17 05:17:38.229744 0.7058823529411765\n",
      "2020-06-17 05:17:46.556416 0.6972083035075162\n",
      "2020-06-17 05:17:54.926216 0.7130550033134525\n",
      "2020-06-17 05:18:03.248206 0.7315968289920725\n",
      "2020-06-17 05:18:11.873083 0.6618392469225199\n",
      "2020-06-17 05:18:20.258610 0.7013527575442249\n",
      "2020-06-17 05:18:29.042557 0.6521452145214522\n",
      "2020-06-17 05:18:37.801146 0.6956521739130435\n",
      "2020-06-17 05:18:46.670734 0.6801152737752162\n",
      "2020-06-17 05:18:55.120270 0.6968503937007875\n",
      "2020-06-17 05:19:03.731645 0.6225806451612903\n",
      "2020-06-17 05:19:12.375540 0.7044534412955465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-17 05:19:21.067313 0.7203947368421053\n",
      "2020-06-17 05:19:30.780348 0.7017543859649122\n",
      "Epoch 00015: early stopping\n",
      "2020-06-17 05:19:51.368592 0.7071895424836602\n",
      "2020-06-17 05:19:51.414143   Min 0.742953777\n",
      "2020-06-17 05:19:51.414540   Max 0.745972739\n",
      "2020-06-17 05:19:51.414732   Avg 0.7441162800666669\n",
      "2020-06-17 05:19:51.414884   Std 0.0009396971237915169\n",
      "2020-06-17 05:19:51.415068   Avg Distance 0.04597701149425287\n",
      "2020-06-17 05:19:51.415132 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:19:51.418605 -- Generation 98 --\n",
      "Epoch 00037: early stopping\n",
      "2020-06-17 05:20:22.491541 0.6838198911429985\n",
      "2020-06-17 05:20:31.386929 0.6936114732724902\n",
      "2020-06-17 05:20:40.198857 0.7071240105540898\n",
      "2020-06-17 05:20:49.014709 0.7085146053449347\n",
      "2020-06-17 05:20:58.256992 0.7041198501872659\n",
      "2020-06-17 05:21:06.939532 0.6365834004834812\n",
      "2020-06-17 05:21:15.741359 0.6989473684210527\n",
      "2020-06-17 05:21:24.841839 0.6890156918687589\n",
      "2020-06-17 05:21:33.357326 0.6695402298850573\n",
      "2020-06-17 05:21:42.420466 0.7207792207792207\n",
      "2020-06-17 05:21:51.378184 0.6750877192982456\n",
      "2020-06-17 05:22:00.258101 0.730077120822622\n",
      "2020-06-17 05:22:09.642638 0.7076710435383552\n",
      "2020-06-17 05:22:18.867044 0.6698610445615716\n",
      "2020-06-17 05:22:28.345288 0.6871896722939425\n",
      "2020-06-17 05:22:37.445310 0.5095785440613028\n",
      "2020-06-17 05:22:46.568162 0.6770538243626062\n",
      "2020-06-17 05:22:55.914869 0.7335950644980371\n",
      "2020-06-17 05:23:05.231431 0.5236768802228413\n",
      "2020-06-17 05:23:14.385753 0.6915254237288135\n",
      "2020-06-17 05:23:14.411649   Min 0.742953777\n",
      "2020-06-17 05:23:14.411986   Max 0.745972739\n",
      "2020-06-17 05:23:14.412005   Avg 0.7445410662000003\n",
      "2020-06-17 05:23:14.412018   Std 0.0007888842357528224\n",
      "2020-06-17 05:23:14.412033   Avg Distance 0.063660477453581\n",
      "2020-06-17 05:23:14.412055 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:23:14.416737 -- Generation 99 --\n",
      "2020-06-17 05:23:23.156598 0.728110599078341\n",
      "2020-06-17 05:23:32.325327 0.6934269944806825\n",
      "Epoch 00020: early stopping\n",
      "2020-06-17 05:23:54.345946 0.6509695290858726\n",
      "2020-06-17 05:24:03.692864 0.709163346613546\n",
      "2020-06-17 05:24:13.004249 0.6666666666666666\n",
      "2020-06-17 05:24:21.283882 0.0\n",
      "Epoch 00011: early stopping\n",
      "2020-06-17 05:24:33.846914 0.0\n",
      "2020-06-17 05:24:43.053566 0.7025906735751295\n",
      "2020-06-17 05:24:52.567643 0.7355689939527214\n",
      "2020-06-17 05:25:02.127989 0.7216035634743875\n",
      "2020-06-17 05:25:11.702639 0.6183889340927583\n",
      "2020-06-17 05:25:21.362902 0.6701030927835052\n",
      "2020-06-17 05:25:30.799810 0.7179209528965891\n",
      "Epoch 00011: early stopping\n",
      "2020-06-17 05:25:46.010342 0.7109207708779444\n",
      "2020-06-17 05:25:55.083188 0.6386167146974063\n",
      "2020-06-17 05:26:04.776195 0.6926147704590818\n",
      "2020-06-17 05:26:14.625136 0.7234762979683972\n",
      "2020-06-17 05:26:25.277626 0.6563467492260061\n",
      "2020-06-17 05:26:35.055151 0.6467065868263473\n",
      "2020-06-17 05:26:44.422156 0.710344827586207\n",
      "2020-06-17 05:26:54.553061 0.650432050274941\n",
      "2020-06-17 05:27:04.627403 0.7186261558784676\n",
      "2020-06-17 05:27:14.691045 0.6993603411513859\n",
      "2020-06-17 05:27:14.715182   Min 0.7442949090000001\n",
      "2020-06-17 05:27:14.715251   Max 0.745972739\n",
      "2020-06-17 05:27:14.715276   Avg 0.7446864026666671\n",
      "2020-06-17 05:27:14.715291   Std 0.0007096425350608558\n",
      "2020-06-17 05:27:14.715305   Avg Distance 0.07117595048629546\n",
      "2020-06-17 05:27:14.715333 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n",
      "2020-06-17 05:27:14.719912 -- Generation 100 --\n",
      "2020-06-17 05:27:24.751804 0.7340107591153615\n",
      "2020-06-17 05:27:34.927075 0.6903270702853166\n",
      "2020-06-17 05:27:44.769827 0.6698610445615716\n",
      "2020-06-17 05:27:54.841357 0.5994919559695173\n",
      "2020-06-17 05:28:05.137508 0.7075208913649024\n",
      "2020-06-17 05:28:15.341385 0.7422324510932107\n",
      "2020-06-17 05:28:25.780595 0.7129877071084981\n",
      "2020-06-17 05:28:35.811778 0.744593804792519\n",
      "2020-06-17 05:28:45.895411 0.6629055007052186\n",
      "2020-06-17 05:28:56.214374 0.7324276432368576\n",
      "2020-06-17 05:29:06.667614 0.7173447537473234\n",
      "2020-06-17 05:29:17.756513 0.7190082644628099\n",
      "2020-06-17 05:29:27.729612 0.6078098471986417\n",
      "2020-06-17 05:29:38.295242 0.6926345609065157\n",
      "2020-06-17 05:29:48.959266 0.6381260096930533\n",
      "2020-06-17 05:29:59.501907 0.69965075669383\n",
      "2020-06-17 05:30:09.470812 0.7178401270513499\n",
      "2020-06-17 05:30:20.198952 0.7047619047619047\n",
      "2020-06-17 05:30:30.964373 0.711392405063291\n",
      "2020-06-17 05:30:41.647514 0.6666666666666666\n",
      "2020-06-17 05:30:52.200518 0.6904761904761905\n",
      "2020-06-17 05:31:03.124684 0.7324434126523505\n",
      "2020-06-17 05:31:03.149587   Min 0.7442949090000001\n",
      "2020-06-17 05:31:03.149685   Max 0.745972739\n",
      "2020-06-17 05:31:03.149703   Avg 0.744696365859751\n",
      "2020-06-17 05:31:03.149724   Std 0.0007061658197973108\n",
      "2020-06-17 05:31:03.149740   Avg Distance 0.07316534040671979\n",
      "2020-06-17 05:31:03.149754 [3, 205, 'nadam', 0.004444097, 0.7888114190000001, 'lecun_normal', 100, 113, 3, 'elu', 'lecun_normal', 1, 264, 4, 'elu', 'glorot_uniform', 5, 419, 5, 'elu', 'ones', 2, 43, 2, 'elu', 'glorot_uniform', 1, 484, 1, 'softsign', 'he_normal', 3, 489, 5, 'softmax', 'ones', 3, 375, 2, 'softplus', 'he_uniform', 4, 65, 5, 'selu', 'he_uniform', 3, 492, 5, 'elu', 'he_uniform', 2, 448, 4, 'linear', 'he_normal', 5, 211, 4, 'softmax', 'he_uniform', 1, 504, 5, 'softmax', 'zeros', 1, 480, 4, 'hard_sigmoid', 'he_uniform', 5, 15, 'selu', 'zeros', 3, 2, 'elu', 'he_normal', 2, 29, 'linear', 'lecun_normal', 5, 19, 'tanh', 'lecun_uniform', 5, 12, 'tanh', 'glorot_uniform', 3, 17, 'selu', 'he_normal', 2, 6, 6, 4, 6, 5, 0.16737655699999998, 0.9150867970000001, 0.04029452, 'embedding_layer', 'convolutional_layer', 'global_maxpooling_layer', 'dropout_layer', 'dense_layer', 'output_layer']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from deap import base\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    globalparameters = []\n",
    "    globalparameters.append((\"epochs\", random.randint, 1, 100))\n",
    "    globalparameters.append((\"batch_size\", random.randint, 32, 256))\n",
    "\n",
    "    globalparameters.append((\"optimizer\", random.choice, ['sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam',\n",
    "                                                          'adamax', 'nadam']))\n",
    "    globalparameters.append((\"learning_rate\", random.uniform, 1e-4, 1e-2))\n",
    "    globalparameters.append((\"momentum\", random.uniform, 0, 1))\n",
    "    globalparameters.append((\"output_init_mode\", random.choice,\n",
    "                             ['zeros',\n",
    "                              'ones',\n",
    "                              'uniform',\n",
    "                              'normal',\n",
    "                              'glorot_normal',\n",
    "                              'glorot_uniform',\n",
    "                              'he_normal',\n",
    "                              'he_uniform',\n",
    "                              'lecun_normal',\n",
    "                              'lecun_uniform']))\n",
    "    globalparameters.append((\"output_dim\", random.choice, [50, 100, 200, 300]))\n",
    "\n",
    "    layerparameters = {}\n",
    "    layerparameters[\"num_filters\"] = [random.randint, 32, 512]\n",
    "    layerparameters[\"kernel_size\"] = [random.randint, 1, 5]\n",
    "    layerparameters[\"conv_activation_func\"] = [random.choice,\n",
    "                                               ['relu', 'softmax', 'elu', 'selu',\n",
    "                                                'softplus', 'softsign', 'tanh',\n",
    "                                                'sigmoid', 'hard_sigmoid', 'linear']]\n",
    "    layerparameters[\"conv_init_mode\"] = [random.choice,\n",
    "                                         ['zeros',\n",
    "                                          'ones',\n",
    "                                          'uniform',\n",
    "                                          'normal',\n",
    "                                          'glorot_normal',\n",
    "                                          'glorot_uniform',\n",
    "                                          'he_normal',\n",
    "                                          'he_uniform',\n",
    "                                          'lecun_normal',\n",
    "                                          'lecun_uniform']]\n",
    "    layerparameters[\"conv_weight_constraint\"] = [random.randint, 1, 5]\n",
    "    layerparameters[\"neurons\"] = [random.randint, 1, 30]\n",
    "    layerparameters[\"dense_activation_func\"] = [random.choice,\n",
    "                                                ['relu', 'softmax', 'elu', 'selu',\n",
    "                                                 'softplus', 'softsign', 'tanh',\n",
    "                                                 'sigmoid', 'hard_sigmoid', 'linear']]\n",
    "    layerparameters[\"dense_init_mode\"] = [random.choice,\n",
    "                                          ['zeros',\n",
    "                                           'ones',\n",
    "                                           'uniform',\n",
    "                                           'normal',\n",
    "                                           'glorot_normal',\n",
    "                                           'glorot_uniform',\n",
    "                                           'he_normal',\n",
    "                                           'he_uniform',\n",
    "                                           'lecun_normal',\n",
    "                                           'lecun_uniform']]\n",
    "    layerparameters[\"dense_weight_constraint\"] = [random.randint, 1, 5]\n",
    "    layerparameters[\"pool_size\"] = [random.randint, 2, 6]\n",
    "    layerparameters[\"dropout_rate\"] = [random.uniform, 0, 1]\n",
    "\n",
    "    defaultVal = collections.OrderedDict([\n",
    "        (\"epochs\", 10),\n",
    "        (\"batch_size\", 32),\n",
    "        (\"optimizer\", \"adam\"),\n",
    "        (\"learning_rate\", 1e-4),\n",
    "        (\"momentum\", 0.9),\n",
    "        (\"output_init_mode\", \"glorot_uniform\"),\n",
    "        (\"output_dim\", 100)]\n",
    "    )\n",
    "    \n",
    "    # object class\n",
    "    util = utility()\n",
    "    toolbox = base.Toolbox()\n",
    "    toolboxes = []\n",
    "\n",
    "    # Attribute generator\n",
    "    for hyper in globalparameters:\n",
    "        if len(hyper) == 3:\n",
    "            toolbox.register(hyper[0], hyper[1], hyper[2])\n",
    "        else:\n",
    "            toolbox.register(hyper[0], hyper[1], hyper[2], hyper[3])\n",
    "\n",
    "    toolboxes.append(toolbox.epochs)\n",
    "    toolboxes.append(toolbox.batch_size)\n",
    "    toolboxes.append(toolbox.optimizer)\n",
    "    toolboxes.append(toolbox.learning_rate)\n",
    "    toolboxes.append(toolbox.momentum)\n",
    "    toolboxes.append(toolbox.output_init_mode)\n",
    "    toolboxes.append(toolbox.output_dim)\n",
    "\n",
    "#     path_ind, max_conv_idx, max_maxpooling_idx, max_dense_idx, max_dropout_idx = generateFSM(n_pop, layerparameters,\n",
    "#                                                                                              toolbox, toolboxes,\n",
    "#                                                                                              defaultVal)\n",
    "\n",
    "    # Read population data\n",
    "    dfPopulation = util.read_CSV(resultsPath + population_path)\n",
    "\n",
    "    path_ind, fitnesses, max_conv_idx, max_maxpooling_idx, max_dense_idx, max_dropout_idx = openFSM(dfPopulation, layerparameters,\n",
    "                                                                                         toolbox, toolboxes, defaultVal)\n",
    "    dfPopulation = dfPopulation.drop(columns=[col for col in dfPopulation if col not in defaultVal])\n",
    "\n",
    "    population = dfPopulation.loc[:, ~dfPopulation.columns.str.match('Unnamed')].values.tolist()\n",
    "\n",
    "    # Read data\n",
    "    dfTraining = util.read_CSV(datasetPath + training_path)\n",
    "    \n",
    "    # Read trial data\n",
    "    dfTrial = util.read_CSV(datasetPath + trial_path)\n",
    "\n",
    "    textsTraining, labelsTraining = util.get_text_label(dfTraining)\n",
    "    textsTrial, labelsTrial = util.get_text_label(dfTrial)\n",
    "    cfold = {}\n",
    "\n",
    "    X_train, X_val, y_train, y_val, vocab_size, maxlen, embedding_matrix = util.get_training_trial_data(\n",
    "        textsTraining, labelsTraining, textsTrial, labelsTrial, glovePath)\n",
    "    cfold= {'X_train': X_train, 'X_val': X_val, 'y_train': y_train, 'y_val': y_val, 'vocab_size': vocab_size,\n",
    "                  'maxlen': maxlen, 'embedding_matrix': embedding_matrix}\n",
    "                  \n",
    "    ga = GeneticAlgorithm(toolbox, toolboxes, cross_rate, mut_rate, n_pop, n_gen, resultsPath, testing_name,\n",
    "                          cfold, globalparameters, layerparameters, defaultVal, path_ind, max_conv_idx, max_maxpooling_idx, max_dense_idx, max_dropout_idx)\n",
    "    \n",
    "    del toolbox, toolboxes, n_pop, n_gen, resultsPath, testing_name, cfold, globalparameters, layerparameters, defaultVal, path_ind, max_conv_idx, max_maxpooling_idx, max_dense_idx, max_dropout_idx\n",
    "    gc.collect()\n",
    "    \n",
    "#     ga.runGA()\n",
    "    ga.runGA(population, fitnesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.session.delete();\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GA-CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
