{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lab/dbms/fatyanosa/.userprogram/anaconda3/envs/GA-CNN/lib/python3.7/site-packages/pyparsing.py:3174: FutureWarning: Possible set intersection at position 3\n",
      "  self.re = re.compile(self.reString)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from os import path\n",
    "import csv\n",
    "import autosklearn.classification\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class utility:\n",
    "\n",
    "    def append_df_to_excel(self, df, excel_path):\n",
    "        if path.isfile(excel_path):\n",
    "            df_excel = pd.read_excel(excel_path)\n",
    "            result = pd.concat([df_excel, df], ignore_index=True)\n",
    "            result.to_excel(excel_path, index=False)\n",
    "        else:\n",
    "            df.to_excel(excel_path, index=False)\n",
    "\n",
    "    def read_CSV(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "\n",
    "    def get_text_label(self, df):\n",
    "        texts = []  # list of text samples\n",
    "        labels = []  # list of label ids\n",
    "        for index, row in df.iterrows():\n",
    "            if isinstance(row['sentence'], float):\n",
    "                texts.append(str(row['sentence']))\n",
    "            else:\n",
    "                texts.append(row['sentence'])\n",
    "\n",
    "            labels.append(row['label'])\n",
    "\n",
    "        return texts, labels\n",
    "\n",
    "    def tokenize_texts(self, texts):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def padding_texts(self, texts, maxlen):\n",
    "\n",
    "        texts = tf.keras.preprocessing.sequence.pad_sequences(texts, padding='post', maxlen=maxlen)\n",
    "\n",
    "        return texts\n",
    "\n",
    "    def get_testing_metric(self, y_test, y_pred):\n",
    "        accuracyScore = accuracy_score(y_test, y_pred)\n",
    "        precisionScore= precision_score(y_test, y_pred)\n",
    "        recallScore = recall_score(y_test, y_pred)\n",
    "        f1Score = f1_score(y_test, y_pred)\n",
    "\n",
    "        return accuracyScore, precisionScore, recallScore, f1Score\n",
    "\n",
    "    def write_df_csv(self, df, out_path):\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "    def create_embedding_matrix(self, filepath, word_index, embedding_dim):\n",
    "        vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "        with open(filepath, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                word, *vector = line.split()\n",
    "                if word in word_index:\n",
    "                    idx = word_index[word]\n",
    "                    embedding_matrix[idx] = np.array(\n",
    "                        vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def get_max_length_of_sentences(self, texts):\n",
    "        maxlength = 0\n",
    "        for text in texts:\n",
    "            if (len(text.split()) > maxlength):\n",
    "                maxlength = len(text.split())\n",
    "\n",
    "        return maxlength\n",
    "\n",
    "    def get_training_trial_data(self, textsTraining, textsTrial, labelsTraining, labelsTrial):\n",
    "        textsTraining, textsTesting = np.asarray(textsTraining), np.asarray(textsTrial)\n",
    "        y_train, y_val = np.asarray(labelsTraining), np.asarray(labelsTrial)\n",
    "\n",
    "        # Tokenize words\n",
    "        tokenizer = self.tokenize_texts(textsTraining)\n",
    "        X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "        X_val = tokenizer.texts_to_sequences(textsTesting)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # get maxlen\n",
    "        maxlen = self.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "        X_train = self.padding_texts(X_train, maxlen)\n",
    "        X_val = self.padding_texts(X_val, maxlen)\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "    def get_X_Y_data(self, textsTraining, labelsTraining):\n",
    "        textsTraining = np.asarray(textsTraining)\n",
    "        y_train = np.asarray(labelsTraining)\n",
    "\n",
    "        # Tokenize words\n",
    "        tokenizer = self.tokenize_texts(textsTraining)\n",
    "        X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # get maxlen\n",
    "        maxlen = self.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "        X_train = self.padding_texts(X_train, maxlen)\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    def Average(self, list):\n",
    "        return sum(list) / len(list)\n",
    "    \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'TrainingData.csv'\n",
    "trial_path = 'TrialData.csv'\n",
    "testing_path = 'EvaluationData.csv'\n",
    "root_path = '/lab/dbms/fatyanosa'\n",
    "datasetPath = '{}/Dataset/Suggestion Mining/'.format(root_path)\n",
    "resultsPath = '{}/Server1/Suggestion Mining/Results/'.format(root_path)\n",
    "testing_name = \"Auto-Sklearn_1hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2020-06-17 14:16:45,690:AutoMLSMBO(1)::73a0658104e872729733e5782040e19b] Could not find meta-data directory /lab/dbms/fatyanosa/.userprogram/anaconda3/envs/GA-CNN/lib/python3.7/site-packages/autosklearn/metalearning/files/f1_score_binary.classification_dense\n",
      "[WARNING] [2020-06-17 15:16:50,051:AutoMLSMBO(1)::73a0658104e872729733e5782040e19b] Could not find meta-data directory /lab/dbms/fatyanosa/.userprogram/anaconda3/envs/GA-CNN/lib/python3.7/site-packages/autosklearn/metalearning/files/f1_score_binary.classification_dense\n",
      "[WARNING] [2020-06-17 16:16:48,994:AutoMLSMBO(1)::73a0658104e872729733e5782040e19b] Could not find meta-data directory /lab/dbms/fatyanosa/.userprogram/anaconda3/envs/GA-CNN/lib/python3.7/site-packages/autosklearn/metalearning/files/f1_score_binary.classification_dense\n",
      "[WARNING] [2020-06-17 17:16:59,537:AutoMLSMBO(1)::73a0658104e872729733e5782040e19b] Could not find meta-data directory /lab/dbms/fatyanosa/.userprogram/anaconda3/envs/GA-CNN/lib/python3.7/site-packages/autosklearn/metalearning/files/f1_score_binary.classification_dense\n",
      "[WARNING] [2020-06-17 18:17:02,618:AutoMLSMBO(1)::73a0658104e872729733e5782040e19b] Could not find meta-data directory /lab/dbms/fatyanosa/.userprogram/anaconda3/envs/GA-CNN/lib/python3.7/site-packages/autosklearn/metalearning/files/f1_score_binary.classification_dense\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    util = utility()\n",
    "    n_run = 5\n",
    "        \n",
    "    # Read data\n",
    "    dfTraining = util.read_CSV(datasetPath + training_path)\n",
    "    \n",
    "    # Read trial data\n",
    "    dfTrial = util.read_CSV(datasetPath + trial_path)\n",
    "    \n",
    "    textsTraining, labelsTraining = util.get_text_label(dfTraining)\n",
    "    textsTrial, labelsTrial = util.get_text_label(dfTrial)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = util.get_training_trial_data(\n",
    "    textsTraining, textsTrial, labelsTraining, labelsTrial)\n",
    "    \n",
    "    # Create Testing Results\n",
    "    f = open(resultsPath + testing_name + \".csv\", \"a+\")\n",
    "    f.write(\"i,score,time\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    scorer = autosklearn.metrics.make_scorer(\n",
    "          'f1_score',\n",
    "          f1_score\n",
    "    )\n",
    "\n",
    "    for i in range(0, n_run):\n",
    "        start_time = timeit.default_timer()      \n",
    "\n",
    "        cls = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=3600)\n",
    "\n",
    "        score = cls.fit(X_train, y_train, metric=scorer).score(X_val, y_val)\n",
    "        pickle.dump(cls, open(resultsPath + testing_name + str(i + 1) + '.pickle', 'wb'))\n",
    "        elapsed = timeit.default_timer() - start_time  \n",
    "\n",
    "        # save testing data\n",
    "        f = open(resultsPath + testing_name + \".csv\", 'a')\n",
    "        f.write(str(i + 1)\n",
    "              + ',' + str(score)\n",
    "              + ',' + str(elapsed)\n",
    "              + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.pipeline import make_pipeline, make_union\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from tpot.builtins import StackingEstimator\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "# from copy import copy\n",
    "# import time\n",
    "\n",
    "# # NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "# # tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# # features = tpot_data.drop('target', axis=1)\n",
    "# # training_features, testing_features, training_target, testing_target = \\\n",
    "# #             train_test_split(features, tpot_data['target'], random_state=None)\n",
    "# util = utility()\n",
    "# n_run = 30\n",
    "\n",
    "# # Read data\n",
    "# dfTraining = util.read_CSV(datasetPath + training_path)\n",
    "\n",
    "# # Read trial data\n",
    "# dfTest = util.read_CSV(datasetPath + testing_path)\n",
    "\n",
    "# textsTraining, labelsTraining = util.get_text_label(dfTraining)\n",
    "# textsTest, labelsTest = util.get_text_label(dfTest)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = util.get_training_test_data(\n",
    "#     textsTraining, textsTest, labelsTraining, labelsTest)\n",
    "\n",
    "# # Create Testing Results\n",
    "# f = open(resultsPath + testing_name + \".csv\", \"w+\")\n",
    "# f.write(\"i,accuracy,precision,recall,f1Score,time\\n\")\n",
    "# f.close()\n",
    "# for i in range(0, n_run):\n",
    "#     then = time.time()\n",
    "#     # Average CV score on the training set was: 0.6534839924670433\n",
    "#     exported_pipeline = make_pipeline(\n",
    "#         make_union(\n",
    "#             FunctionTransformer(copy),\n",
    "#             RobustScaler()\n",
    "#         ),\n",
    "#         StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.1, max_depth=2, max_features=0.6500000000000001, min_samples_leaf=17, min_samples_split=17, n_estimators=100, subsample=0.6000000000000001)),\n",
    "#         StackingEstimator(estimator=LinearSVC(C=0.0001, dual=True, loss=\"hinge\", penalty=\"l2\", tol=0.1)),\n",
    "#         StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.1, max_depth=7, max_features=0.8500000000000001, min_samples_leaf=6, min_samples_split=17, n_estimators=100, subsample=0.9000000000000001)),\n",
    "#         RobustScaler(),\n",
    "#         BernoulliNB(alpha=10.0, fit_prior=False)\n",
    "#     )\n",
    "\n",
    "#     exported_pipeline.fit(X_train, y_train)\n",
    "#     y_pred = exported_pipeline.predict(X_test)\n",
    "\n",
    "#     # CNN metrics\n",
    "#     accuracyScore, precisionScore, recallScore, f1Score = util.get_testing_metric(y_test, y_pred)\n",
    "\n",
    "#     now = time.time()\n",
    "#     diff = now - then\n",
    "#     print(diff)\n",
    "#     print(f1Score)\n",
    "\n",
    "#     # save testing data\n",
    "#     f = open(resultsPath + testing_name + \".csv\", 'a')\n",
    "#     f.write(str(i + 1)\n",
    "#             + ',' + str(accuracyScore)\n",
    "#             + ',' + str(precisionScore)\n",
    "#             + ',' + str(recallScore)\n",
    "#             + ',' + str(f1Score)\n",
    "#             + ',' + str(diff) + '\\n')\n",
    "#     f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
